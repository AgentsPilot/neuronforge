/**
 * Declarative IR Compiler
 *
 * This compiler is SMART - it infers execution details from declarative intent.
 *
 * Key Intelligence:
 * - Infers loops from delivery_rules patterns
 * - Generates operation IDs automatically
 * - Manages variable flow
 * - Auto-injects missing transforms (e.g., attachment extraction)
 * - Binds to appropriate plugins
 *
 * Design Philosophy:
 * The IR says WHAT → The compiler figures out HOW
 */

import type { DeclarativeLogicalIR, AIOperation, DataSource } from '../logical-ir/schemas/declarative-ir-types'
import type { WorkflowStep } from '@/lib/pilot/types/pilot-dsl-types'
import { validateDeclarativeIR } from '../logical-ir/validation/DeclarativeIRValidator'
import type { PluginManagerV2 } from '@/lib/server/plugin-manager-v2'
import { PluginResolver } from './utils/PluginResolver'
import { compilerMetrics } from './CompilerMetrics'
import { analyzeOutputSchema } from '@/lib/pilot/utils/SchemaAwareDataExtractor'

// ============================================================================
// Type Definitions
// ============================================================================

export interface CompilationResult {
  success: boolean
  workflow: WorkflowStep[]
  logs: string[]
  errors?: string[]
  plugins_used?: string[]
  compilation_time_ms?: number
  ir?: DeclarativeLogicalIR  // Store IR for Phase 5 DSL generation
}

// ============================================================================
// Compiler Context - Tracks state during compilation
// ============================================================================

interface CompilerContext {
  currentVariable: string // Current data variable in the pipeline
  stepCounter: number // For generating unique step IDs
  warnings: string[]
  logs: string[]
  dataSourceConfigs: Map<string, any> // Track configs from read operations for reuse in write operations
  primaryDataSourcePlugin?: string // Plugin name of the primary data source (for column mapping)
  primaryDataSourceOperation?: string // Operation name of the primary data source
  ir: DeclarativeLogicalIR // Full IR for access to destination configs
  referenceDataVariable?: string // Variable path for reference data (for conditional headers)
  aiOutputFields?: string[] // Field names generated by AI operations (for column mapping)
  tabularDataVariable?: string | null // Variable for 2D array data (spreadsheet-like destinations)
  contentDataVariable?: string | null // Variable for rendered HTML data (email/messaging destinations)
}

// ============================================================================
// Declarative Compiler Class
// ============================================================================

export class DeclarativeCompiler {
  private pluginResolver: PluginResolver

  constructor(pluginManager?: PluginManagerV2) {
    this.pluginResolver = new PluginResolver(pluginManager)
    console.log('[DeclarativeCompiler] Initialized with PluginResolver')
  }

  /**
   * Compile declarative IR to executable PILOT DSL
   */
  async compile(ir: DeclarativeLogicalIR): Promise<CompilationResult> {
    const startTime = Date.now()
    console.log('[DeclarativeCompiler] Starting compilation...')
    console.log('[DeclarativeCompiler] Goal:', ir.goal)

    // Detect features for metrics
    const features = {
      hasFilters: !!(ir.filters && (ir.filters.conditions || ir.filters.groups)),
      hasAI: !!(ir.ai_operations && ir.ai_operations.length > 0),
      hasDeduplication: ir.data_sources.some(ds => this.isReferenceDataSource(ds)),
      hasGrouping: !!ir.grouping,
      hasPartitions: !!(ir.partitions && ir.partitions.length > 0),
      multiDestination: !!(ir.delivery_rules.multiple_destinations && ir.delivery_rules.multiple_destinations.length > 1)
    }

    // Step 1: Validate IR
    const validation = validateDeclarativeIR(ir)
    if (!validation.valid) {
      console.log('[DeclarativeCompiler] ✗ IR validation failed')
      console.log('[DeclarativeCompiler] Validation errors:', JSON.stringify(validation.errors, null, 2))

      // Record failure metric
      compilerMetrics.record({
        timestamp: new Date(),
        success: false,
        irVersion: ir.ir_version,
        patternType: 'validation_failed',
        stepCount: 0,
        compilationTimeMs: Date.now() - startTime,
        errorType: 'validation_error',
        errorMessage: validation.errors[0]?.message,
        features
      })

      return {
        success: false,
        errors: validation.errors.map(e => e.message),
        logs: [],
        workflow: []
      }
    }

    console.log('[DeclarativeCompiler] ✓ IR validation passed')

    // Step 1.5: Semantic validation - Check for ungrounded fields
    const semanticIssues: string[] = []

    // Check filter conditions for null fields
    if (ir.filters) {
      if (ir.filters.conditions) {
        ir.filters.conditions.forEach((cond, idx) => {
          if (cond.field === null) {
            semanticIssues.push(`Filter condition ${idx + 1} has null field - cannot compile deterministically (requires LLM to infer field)`)
          }
        })
      }
      if (ir.filters.groups) {
        ir.filters.groups.forEach((group, groupIdx) => {
          group.conditions.forEach((cond, condIdx) => {
            if (cond.field === null) {
              semanticIssues.push(`Filter group ${groupIdx + 1}, condition ${condIdx + 1} has null field - cannot compile deterministically (requires LLM to infer field)`)
            }
          })
        })
      }
    }

    if (semanticIssues.length > 0) {
      console.log('[DeclarativeCompiler] ✗ Semantic validation failed - IR contains ungrounded fields')
      console.log('[DeclarativeCompiler] Issues:', semanticIssues)

      // Record failure metric
      compilerMetrics.record({
        timestamp: new Date(),
        success: false,
        irVersion: ir.ir_version,
        patternType: 'ungrounded_fields',
        stepCount: 0,
        compilationTimeMs: Date.now() - startTime,
        errorType: 'semantic_validation_error',
        errorMessage: semanticIssues[0],
        features
      })

      return {
        success: false,
        errors: [
          'IR contains ungrounded fields that require LLM compilation:',
          ...semanticIssues,
          'Tip: Check clarifications_required in the IR for details on missing grounded facts'
        ],
        logs: [],
        workflow: []
      }
    }

    console.log('[DeclarativeCompiler] ✓ Semantic validation passed')

    // Step 2: Initialize compilation context
    const ctx: CompilerContext = {
      currentVariable: 'raw_data',
      stepCounter: 1,
      warnings: [],
      logs: [],
      dataSourceConfigs: new Map(),
      ir // Store IR for access to destination configs
    }

    const steps: WorkflowStep[] = []

    try {
      // Step 3: Compile PRIMARY data sources (read operations)
      this.log(ctx, 'Compiling primary data sources...')
      const dataSteps = this.compileDataSources(ir, ctx)
      steps.push(...dataSteps)

      // Step 3.5: Detect and compile cross-data-source deduplication
      // Priority 1: Explicit reference/lookup data source
      let referenceSource = ir.data_sources.find(ds => this.isReferenceDataSource(ds))

      // Priority 2: Wave 9 - If there's a duplicate_records edge case AND a destination data source,
      // the destination is used as the reference for deduplication (read before write pattern)
      if (!referenceSource && this.hasDuplicateRecordsEdgeCase(ir)) {
        // Find data source with role containing "destination" or "deduplication"
        // LLM may generate: "destination", "deduplication_and_destination", "destination_and_lookup", etc.
        const destinationSource = ir.data_sources.find(ds => {
          const roleLower = ds.role?.toLowerCase() || ''
          return roleLower.includes('destination') || roleLower.includes('deduplication') || roleLower.includes('dedupe')
        })
        if (destinationSource) {
          this.log(ctx, `Wave 9: duplicate_records edge case detected with destination '${destinationSource.source}' (role: ${destinationSource.role}) - enabling deduplication`)
          // Create a modified reference source with read operation for deduplication
          // Use generic 'read' operation - PluginResolver will map to plugin-specific operation
          // (e.g., read_range for google-sheets, list for CRM, search for email, etc.)
          referenceSource = {
            ...destinationSource,
            operation_type: 'read' // Generic read - plugin resolver handles specifics
          }
        }
      }

      if (referenceSource) {
        this.log(ctx, `Detected reference data source (role: "${referenceSource.role}"): ${referenceSource.source} - compiling deduplication pattern`)
        const dedupSteps = this.compileDeduplication(ir, referenceSource, ctx)
        steps.push(...dedupSteps)
      }

      // Step 4: Compile filters
      if (ir.filters && (ir.filters.conditions || ir.filters.groups)) {
        const conditionCount = (ir.filters.conditions?.length || 0) + (ir.filters.groups?.length || 0)
        this.log(ctx, `Compiling ${conditionCount} filter conditions...`)
        const filterSteps = this.compileFilters(ir, ctx)
        steps.push(...filterSteps)
      }

      // Step 5: Infer and compile loops from delivery_rules
      this.log(ctx, 'Analyzing delivery_rules to infer execution pattern...')
      const deliverySteps = this.compileDeliveryPattern(ir, ctx)
      steps.push(...deliverySteps)

      // Step 6: Compile WRITE operations (append/update to destinations)
      this.log(ctx, 'Compiling write operations...')
      const writeSteps = this.compileWriteOperations(ir, ctx)
      steps.push(...writeSteps)

      console.log('[DeclarativeCompiler] ✓ Compilation successful')
      console.log('[DeclarativeCompiler] Generated', steps.length, 'steps')

      // Step 7: Apply execution constraints (retry, timeout) to steps
      if (ir.execution_constraints) {
        this.log(ctx, 'Applying execution constraints to steps...')
        this.applyExecutionConstraints(steps, ir.execution_constraints, ctx)
      }

      // Step 8: Compile edge case handlers
      if (ir.edge_cases && ir.edge_cases.length > 0) {
        this.log(ctx, `Compiling ${ir.edge_cases.length} edge case handlers...`)
        // Edge cases are applied as on_error handlers to steps
        this.applyEdgeCaseHandlers(steps, ir.edge_cases, ctx)
      }

      // Step 9: Log warning for ignored IR fields
      this.logIgnoredIRFields(ir, ctx)

      // Step 10: Normalize step inputs (fix common data type mismatches)
      this.log(ctx, 'Normalizing step inputs to prevent runtime type errors...')
      const normalizedSteps = this.normalizeStepInputs(steps, ctx)

      // Determine pattern type
      const patternType = this.detectPatternType(ir, features)

      // Record success metric
      compilerMetrics.record({
        timestamp: new Date(),
        success: true,
        irVersion: ir.ir_version,
        patternType,
        stepCount: normalizedSteps.length,
        compilationTimeMs: Date.now() - startTime,
        features
      })

      // Extract plugins used from the compiled steps
      const pluginsUsed = this.extractPluginsUsed(normalizedSteps)
      const compilationTime = Date.now() - startTime

      // Return compilation result with IR (DSL will be generated in Phase 5)
      return {
        success: true,
        workflow: normalizedSteps,
        plugins_used: pluginsUsed,
        compilation_time_ms: compilationTime,
        warnings: ctx.warnings,
        logs: ctx.logs,
        ir: ir  // Store IR for Phase 5 DSL generation
      }
    } catch (error) {
      const errorMessage = error instanceof Error ? error.message : 'Unknown compilation error'
      console.error('[DeclarativeCompiler] ✗ Compilation failed:', errorMessage)

      // Record failure metric
      compilerMetrics.record({
        timestamp: new Date(),
        success: false,
        irVersion: ir.ir_version,
        patternType: 'compilation_error',
        stepCount: 0,
        compilationTimeMs: Date.now() - startTime,
        errorType: 'compilation_error',
        errorMessage,
        features
      })

      return {
        success: false,
        errors: [errorMessage],
        warnings: ctx.warnings
      }
    }
  }

  // ==========================================================================
  // Data Source Compilation
  // ==========================================================================

  /**
   * Compile PRIMARY data sources (read operations only)
   * Filters out write_target and reference/lookup sources - those are compiled separately
   */
  private compileDataSources(ir: DeclarativeLogicalIR, ctx: CompilerContext): WorkflowStep[] {
    const steps: WorkflowStep[] = []

    // Filter for primary data sources only
    // Exclude:
    // - Reference/lookup data sources (compiled separately in deduplication)
    // - Write operations (compiled separately in write operations)
    const readSources = ir.data_sources.filter(ds => {
      // Exclude reference/lookup data sources
      if (this.isReferenceDataSource(ds)) {
        return false
      }

      // Exclude write operations
      const writeOperationTypes = ['write', 'append', 'update', 'upsert', 'delete']
      if (ds.operation_type && writeOperationTypes.includes(ds.operation_type)) {
        return false
      }

      // Include everything else (primary, or no role specified)
      return true
    })

    if (readSources.length === 0) {
      this.log(ctx, '⚠ No primary data sources found')
      return steps
    }

    // For now, we'll compile the first primary source as the main pipeline
    // TODO: Support multiple primary sources with joins
    const primarySource = readSources.find(ds =>
      ds.role?.toLowerCase() === 'primary'
    ) || readSources[0] // Fallback to first read source if no explicit "primary" role

    if (!primarySource) {
      this.log(ctx, '⚠ No primary data source found after filtering')
      return steps
    }

    // Compile primary data source
    const primarySteps = this.compileSingleDataSource(ir, primarySource, ctx)
    steps.push(...primarySteps)

    return steps
  }

  /**
   * Compile a single data source (read operation)
   */
  private compileSingleDataSource(ir: DeclarativeLogicalIR, dataSource: any, ctx: CompilerContext): WorkflowStep[] {
    const steps: WorkflowStep[] = []

    if (dataSource.type === 'tabular') {
      // Tabular data source - use PluginResolver with plugin_key and operation_type
      // This works for ANY tabular plugin (google-sheets, airtable, excel, notion, etc.)

      // Determine plugin key (must be provided in IR)
      if (!dataSource.plugin_key) {
        throw new Error(`data_source.plugin_key is required for tabular data source: ${dataSource.source}`)
      }

      const pluginKey = dataSource.plugin_key
      const operationType = dataSource.operation_type || 'read' // Default to read for tabular

      // Resolve plugin with error handling
      let resolution
      try {
        resolution = this.pluginResolver.resolveDataSource(pluginKey, operationType)
      } catch (error) {
        const errorMsg = `Failed to resolve tabular data source plugin: ${pluginKey}.${operationType}`
        this.log(ctx, `✗ ${errorMsg}`)
        throw new Error(`${errorMsg}: ${error instanceof Error ? error.message : String(error)}`)
      }

      console.log('[DeclarativeCompiler] Tabular data source resolution:', {
        source: dataSource.source,
        plugin: resolution.plugin_name,
        action: resolution.operation,  // Use 'action' for PILOT executor compatibility
        operationType
      })

      // Build params from parameter schema and data source
      const params: any = this.buildDataSourceParams(dataSource, resolution.parameters_schema, ctx)

      const metadata = this.generateStepMetadata('read_tabular', 'Read Tabular Data', ctx)
      steps.push({
        ...metadata,
        type: 'action',
        plugin: resolution.plugin_name,
        action: resolution.operation,  // Use 'action' for PILOT executor compatibility
        params,
        output_schema: resolution.output_schema  // Propagate for schema-aware execution
      })

      // ✅ FIX: Use plugin's output_schema to find the correct array field
      // Different plugins return arrays in different nested fields (e.g., "values", "emails", "records")
      const tabularArrayField = this.getOutputArrayFieldName(resolution.plugin_name, resolution.operation)
      ctx.currentVariable = tabularArrayField !== 'data'
        ? `${metadata.id}.data.${tabularArrayField}`
        : `${metadata.id}.data`
      ctx.primaryDataSourcePlugin = resolution.plugin_name
      ctx.primaryDataSourceOperation = resolution.operation
      this.log(ctx, `✓ Using plugin operation: ${resolution.plugin_name}.${resolution.operation} (array field: ${tabularArrayField})`)

      // ✅ FIX: Detect if plugin returns 2D array based on output_schema
      // 2D arrays have rows as arrays, not objects, so field-based access fails
      const { is2DArray: tabularIs2DArray } = this.detectOutputIs2DArray(resolution.plugin_name, resolution.operation)
      if (tabularIs2DArray) {
        const convertMetadata = this.generateStepMetadata('convert_to_objects', 'Convert Rows to Objects', ctx)
        steps.push({
          ...convertMetadata,
          type: 'transform',
          operation: 'rows_to_objects',
          input: `{{${ctx.currentVariable}}}`,
          config: {}
        })
        ctx.currentVariable = `${convertMetadata.id}.data`
        this.log(ctx, `✓ Added rows_to_objects transform for 2D array (stored in: ${ctx.currentVariable})`)
      }

      // Normalize headers if required
      if (ir.normalization?.required_headers) {
        const normMetadata = this.generateStepMetadata('normalize_headers', 'Normalize Headers', ctx)
        steps.push({
          ...normMetadata,
          type: 'transform',
          operation: 'map_headers',
          input: `{{${ctx.currentVariable}}}`,  // Already includes the array field path
          config: {
            required_headers: ir.normalization.required_headers
          }
        })

        // Transform step outputs are stored in .data
        ctx.currentVariable = `${normMetadata.id}.data`
      }
    } else if (dataSource.type === 'api') {
      // API data source - use PluginResolver with plugin_key and operation_type
      // This works for ANY API plugin (gmail, outlook, slack, etc.)

      // Determine plugin key (must be provided in IR)
      if (!dataSource.plugin_key) {
        throw new Error(`data_source.plugin_key is required for API data source: ${dataSource.source}`)
      }

      const pluginKey = dataSource.plugin_key
      const operationType = dataSource.operation_type || 'search' // Default to search for APIs

      // Resolve plugin with error handling
      let resolution
      try {
        resolution = this.pluginResolver.resolveDataSource(pluginKey, operationType)
      } catch (error) {
        const errorMsg = `Failed to resolve API data source plugin: ${pluginKey}.${operationType}`
        this.log(ctx, `✗ ${errorMsg}`)
        throw new Error(`${errorMsg}: ${error instanceof Error ? error.message : String(error)}`)
      }

      console.log('[DeclarativeCompiler] API data source resolution:', {
        source: dataSource.source,
        plugin: resolution.plugin_name,
        action: resolution.operation,  // Use 'action' for PILOT executor compatibility
        operationType
      })

      // Build params from parameter schema and data source
      const params: any = this.buildDataSourceParams(dataSource, resolution.parameters_schema, ctx)

      // ✨ SMART COMPILER: Auto-enable attachments if AI operations need them
      // Generic detection - works for ANY file type (PDF, images, documents, etc.)
      if (ir.ai_operations && ir.ai_operations.length > 0) {
        const needsAttachments = ir.ai_operations.some(op =>
          this.getContextString(op.context).toLowerCase().includes('attachment') ||
          op.instruction?.toLowerCase().includes('attachment')
        )

        if (needsAttachments && params.include_attachments === false) {
          params.include_attachments = true
          this.log(ctx, '✓ Auto-enabled include_attachments for AI attachment processing')
        }
      }

      const metadata = this.generateStepMetadata(`fetch_${dataSource.source}`, `Fetch ${dataSource.source} Data`, ctx)
      steps.push({
        ...metadata,
        type: 'action',
        plugin: resolution.plugin_name,
        action: resolution.operation,  // Use 'action' for PILOT executor compatibility
        params,
        output_schema: resolution.output_schema  // Propagate for schema-aware execution
      })

      // ✅ FIX: Use plugin's output_schema to find the correct array field
      // Different plugins return arrays in different nested fields (e.g., "emails", "messages", "records")
      const apiArrayField = this.getOutputArrayFieldName(resolution.plugin_name, resolution.operation)
      ctx.currentVariable = apiArrayField !== 'data'
        ? `${metadata.id}.data.${apiArrayField}`
        : `${metadata.id}.data`
      ctx.primaryDataSourcePlugin = resolution.plugin_name
      ctx.primaryDataSourceOperation = resolution.operation
      this.log(ctx, `✓ Using plugin operation: ${resolution.plugin_name}.${resolution.operation} (array field: ${apiArrayField})`)

      // ✅ FIX: Detect if plugin returns 2D array based on output_schema
      // Most API sources return objects, but check for consistency
      const { is2DArray: apiIs2DArray } = this.detectOutputIs2DArray(resolution.plugin_name, resolution.operation)
      if (apiIs2DArray) {
        const convertMetadata = this.generateStepMetadata('convert_to_objects', 'Convert Rows to Objects', ctx)
        steps.push({
          ...convertMetadata,
          type: 'transform',
          operation: 'rows_to_objects',
          input: `{{${ctx.currentVariable}}}`,
          config: {}
        })
        ctx.currentVariable = `${convertMetadata.id}.data`
        this.log(ctx, `✓ Added rows_to_objects transform for 2D array (stored in: ${ctx.currentVariable})`)
      }
    }

    this.log(ctx, `✓ Compiled data source: ${dataSource.type} from ${dataSource.source}`)
    return steps
  }

  // ==========================================================================
  // Deduplication Compilation (Cross-Data-Source)
  // ==========================================================================

  /**
   * Compile cross-data-source deduplication pattern
   *
   * Pattern: Primary data source (e.g., new emails) vs Reference data source (e.g., existing sheet rows)
   *
   * Strategy:
   * 1. Read reference data source (existing records)
   * 2. Use scatter_gather to loop over primary data (new items)
   * 3. For each new item, filter reference data to find duplicates
   * 4. Use gather to collect only non-duplicate items
   *
   * This generates a deterministic deduplication workflow without LLM guessing
   */
  private compileDeduplication(
    ir: DeclarativeLogicalIR,
    referenceSource: DataSource,
    ctx: CompilerContext
  ): WorkflowStep[] {
    const steps: WorkflowStep[] = []

    // Save the current primary data variable
    const primaryDataVariable = ctx.currentVariable

    // Step 1: Read the reference data source (e.g., existing sheet with processed IDs)
    this.log(ctx, `Reading reference data source: ${referenceSource.source}`)

    if (!referenceSource.plugin_key) {
      throw new Error(`Reference data source missing plugin_key: ${referenceSource.source}`)
    }

    const pluginKey = referenceSource.plugin_key
    const operationType = referenceSource.operation_type || 'read'

    // Resolve plugin with error handling
    let resolution
    try {
      resolution = this.pluginResolver.resolveDataSource(pluginKey, operationType)
    } catch (error) {
      const errorMsg = `Failed to resolve reference data source plugin: ${pluginKey}.${operationType}`
      this.log(ctx, `✗ ${errorMsg}`)
      throw new Error(`${errorMsg}: ${error instanceof Error ? error.message : String(error)}`)
    }

    console.log('[DeclarativeCompiler] Reference data source resolution:', {
      source: referenceSource.source,
      plugin: resolution.plugin_name,
      action: resolution.operation,  // Use 'action' for PILOT executor compatibility
      operationType
    })

    const params: any = this.buildDataSourceParams(referenceSource, resolution.parameters_schema, ctx)

    // Store ALL params for reuse in write operations (plugin-agnostic)
    // Any params from read operation can be reused by write operations targeting the same source
    ctx.dataSourceConfigs.set(referenceSource.source, { ...params })
    this.log(ctx, `✓ Stored config for data source '${referenceSource.source}': ${Object.keys(params).join(', ')}`)

    const refMetadata = this.generateStepMetadata('read_reference', `Read ${referenceSource.source}`, ctx)
    steps.push({
      ...refMetadata,
      type: 'action',
      plugin: resolution.plugin_name,
      action: resolution.operation,  // Use 'action' for PILOT executor compatibility
      params,
      output_schema: resolution.output_schema  // Propagate for schema-aware execution
    })

    // ✅ FIX: Use plugin's output_schema to find the correct array field
    // Different plugins return arrays in different nested fields (e.g., "values", "emails", "records")
    // The getOutputArrayFieldName method reads the plugin's output_schema dynamically
    const arrayField = this.getOutputArrayFieldName(resolution.plugin_name, resolution.operation)
    let referenceDataVariable = arrayField !== 'data'
      ? `${refMetadata.id}.data.${arrayField}`
      : `${refMetadata.id}.data`
    this.log(ctx, `✓ Reference data stored in: ${referenceDataVariable} (array field: ${arrayField})`)

    // ✅ Store the raw reference data variable for conditional headers
    // This is used in "Prepare Sheets Data" to only add headers when sheet is empty
    ctx.referenceDataVariable = referenceDataVariable

    // ✅ FIX: Detect if plugin returns 2D array (like Google Sheets) based on output_schema
    // 2D arrays have rows as arrays, not objects, so field-based access like row.id fails
    // We add a rows_to_objects transform to convert to object array using first row as headers
    const { is2DArray } = this.detectOutputIs2DArray(resolution.plugin_name, resolution.operation)
    if (is2DArray) {
      const convertMetadata = this.generateStepMetadata('convert_to_objects', 'Convert Rows to Objects', ctx)
      steps.push({
        ...convertMetadata,
        type: 'transform',
        operation: 'rows_to_objects',
        input: `{{${referenceDataVariable}}}`,
        config: {}
      })
      referenceDataVariable = `${convertMetadata.id}.data`
      this.log(ctx, `✓ Added rows_to_objects transform for 2D array output (stored in: ${referenceDataVariable})`)
    }

    // Step 2: Extract identifier field(s) from reference data
    // Supports both single-field and multi-field deduplication
    let identifierFields: string[] = []

    // Check for identifier_field (single) or identifier_fields (multi) in config
    if (referenceSource.config) {
      if (referenceSource.config.identifier_fields && Array.isArray(referenceSource.config.identifier_fields)) {
        // Multi-field deduplication (composite key)
        identifierFields = referenceSource.config.identifier_fields
        this.log(ctx, `Using multi-field deduplication: ${identifierFields.join(', ')}`)
      } else if (referenceSource.config.identifier_field) {
        // Single-field deduplication
        identifierFields = [referenceSource.config.identifier_field]
      }
    }

    // Default to 'id' if no identifier specified
    if (identifierFields.length === 0) {
      identifierFields = ['id']
    }

    // Extract identifier(s) - create composite key if multiple fields
    const extractMetadata = this.generateStepMetadata('extract_existing_ids', 'Extract Existing IDs', ctx)

    // ✅ FIX: In StepExecutor.transformMap with expression, 'item' is the WHOLE array
    // So we need to use item.map() to iterate over each element
    let extractExpression: string
    if (identifierFields.length === 1) {
      // Single field: just extract the value from each row
      extractExpression = `item.map(row => row.${identifierFields[0]})`
      this.log(ctx, `✓ Extracting single identifier field: ${identifierFields[0]}`)
    } else {
      // Multiple fields: create composite key as string "field1|field2|field3"
      const fieldReferences = identifierFields.map(f => `row.${f}`).join(' + "|" + ')
      extractExpression = `item.map(row => ${fieldReferences})`
      this.log(ctx, `✓ Extracting composite key from fields: ${identifierFields.join(', ')}`)
    }

    steps.push({
      ...extractMetadata,
      type: 'transform',
      operation: 'map',
      input: `{{${referenceDataVariable}}}`,
      config: {
        expression: extractExpression
      }
    })

    // For transform steps, the result is in .data (StepOutput structure)
    const existingIdsVariable = `${extractMetadata.id}.data`
    this.log(ctx, `✓ Extracted existing IDs/keys from reference data (stored in: ${existingIdsVariable})`)

    // Step 3: Filter primary data to exclude items whose ID exists in reference
    // Use pre-computed boolean pattern for safety (handles null, uses standard operators)
    // Pattern: Map (compute [item, boolean]) → Filter (item[1] == true) → Map (extract item[0])

    // Step 3a: Pre-compute membership test with null safety
    // Build item key expression (single field or composite)
    // ✅ FIX: In expression context, 'item' is the whole array, 'row' is each element
    let rowKeyExpression: string
    if (identifierFields.length === 1) {
      // Single field
      rowKeyExpression = `row.${identifierFields[0]}`
    } else {
      // Composite key - same format as extraction: "field1|field2|field3"
      rowKeyExpression = identifierFields.map(f => `row.${f}`).join(' + "|" + ')
    }

    const precomputeMetadata = this.generateStepMetadata('precompute_dedup', 'Pre-compute Deduplication Check', ctx)
    steps.push({
      ...precomputeMetadata,
      type: 'transform',
      operation: 'map',
      input: `{{${primaryDataVariable}}}`,
      config: {
        // Null safety: (existingIds || []) handles empty lookup sheet
        // Supports both single-field and multi-field (composite key) deduplication
        // ✅ FIX: 'item' is whole array, use item.map() to transform each element
        expression: `item.map(row => [row, !({{${existingIdsVariable}}} || []).includes(${rowKeyExpression})])`
      }
    })
    this.log(ctx, `✓ Pre-computed deduplication check with null safety`)

    // Step 3b: Filter on boolean (simple comparison, no method calls)
    const dedupMetadata = this.generateStepMetadata('filter_new_items', 'Filter New Items Only', ctx)
    steps.push({
      ...dedupMetadata,
      type: 'transform',
      operation: 'filter',
      input: `{{${precomputeMetadata.id}.data}}`,  // Transform outputs are in .data
      config: {
        condition: `item[1] == true`  // Simple string expression (filter iterates per-item)
      }
    })
    this.log(ctx, `✓ Filtered to new items only`)

    // Step 3c: Extract original items
    const extractItemsMetadata = this.generateStepMetadata('extract_new_items', 'Extract Original Items', ctx)
    steps.push({
      ...extractItemsMetadata,
      type: 'transform',
      operation: 'map',
      input: `{{${dedupMetadata.id}.data}}`,  // Transform outputs are in .data
      config: {
        // ✅ FIX: 'item' is whole array, use item.map() to extract first element from each tuple
        expression: `item.map(row => row[0])`
      }
    })

    // Transform step outputs are stored in .data
    ctx.currentVariable = `${extractItemsMetadata.id}.data`
    this.log(ctx, `✓ ID-based deduplication complete - new items stored in: ${ctx.currentVariable}`)

    // Step 4 (Optional): Time-window deduplication
    // If time_window_hours is specified, add additional filter to skip recently processed items
    if (referenceSource.config && referenceSource.config.time_window_hours) {
      const timeWindowHours = referenceSource.config.time_window_hours
      const timestampField = referenceSource.config.timestamp_field || 'processed_at'

      this.log(ctx, `Adding time-window deduplication: skip if processed within ${timeWindowHours} hours`)

      // Calculate cutoff timestamp (current time - window)
      const cutoffTimestamp = `Date.now() - (${timeWindowHours} * 60 * 60 * 1000)`

      // Pre-compute time check
      const timePrecomputeMetadata = this.generateStepMetadata('precompute_time_check', 'Check Time Window', ctx)
      steps.push({
        ...timePrecomputeMetadata,
        type: 'transform',
        operation: 'map',
        input: `{{${ctx.currentVariable}}}`,
        config: {
          // Check if item was processed recently (within time window)
          // Returns [row, isWithinWindow] for each row
          // ✅ FIX: 'item' is whole array, use item.map()
          expression: `item.map(row => [row, new Date(row.${timestampField}).getTime() > (${cutoffTimestamp})])`
        }
      })

      // Filter out items within time window (keep only old or unprocessed)
      const timeFilterMetadata = this.generateStepMetadata('filter_time_window', 'Filter by Time Window', ctx)
      steps.push({
        ...timeFilterMetadata,
        type: 'transform',
        operation: 'filter',
        input: `{{${timePrecomputeMetadata.id}.data}}`,  // Transform outputs are in .data
        config: {
          condition: `item[1] == false`  // Keep items NOT within window (filter iterates per-item)
        }
      })

      // Extract original items
      const timeExtractMetadata = this.generateStepMetadata('extract_after_time_filter', 'Extract Items After Time Filter', ctx)
      steps.push({
        ...timeExtractMetadata,
        type: 'transform',
        operation: 'map',
        input: `{{${timeFilterMetadata.id}.data}}`,  // Transform outputs are in .data
        config: {
          // ✅ FIX: 'item' is whole array, use item.map()
          expression: `item.map(row => row[0])`
        }
      })

      // Transform step outputs are stored in .data
      ctx.currentVariable = `${timeExtractMetadata.id}.data`
      this.log(ctx, `✓ Time-window deduplication complete - filtered items not processed in last ${timeWindowHours} hours`)
    }

    return steps
  }

  // ==========================================================================
  // Filter Compilation
  // ==========================================================================

  private compileFilters(ir: DeclarativeLogicalIR, ctx: CompilerContext): WorkflowStep[] {
    const steps: WorkflowStep[] = []
    const filterGroup = ir.filters!

    // Helper: Check if a filter value is a placeholder that will never match
    const isPlaceholderValue = (value: any): boolean => {
      if (typeof value !== 'string') return false
      // Detect common placeholder patterns: <topic>, <user>, {query}, etc.
      // These are generated by LLM when it doesn't have a concrete value
      const placeholderPattern = /^<[a-z_]+>$|^\{[a-z_]+\}$/i
      return placeholderPattern.test(value.trim())
    }

    // Helper: Check if a filter condition is a no-op (would always pass) or uses a placeholder
    const isNoOpCondition = (condition: { operator: string; value?: any; field?: string }): boolean => {
      const { operator, value, field } = condition

      // Check for placeholder values that would match nothing
      if (isPlaceholderValue(value)) {
        this.log(ctx, `⚠️ Detected placeholder filter value "${value}" for field "${field}" - skipping (would match nothing)`)
        return true
      }

      // "contains ''" is always true for non-null strings - skip this filter
      if (operator === 'contains' && (value === '' || value === null || value === undefined)) {
        return true
      }

      // "not_contains ''" is always false for non-null strings - this would filter everything
      // We should warn but not skip (user might intentionally want this)

      // "is_not_empty" with no value is valid - NOT a no-op
      // "is_empty" with no value is valid - NOT a no-op

      return false
    }

    // Handle simple conditions with AND/OR logic
    if (filterGroup.conditions && filterGroup.conditions.length > 0) {
      const combineWith = filterGroup.combineWith || 'AND'

      if (combineWith === 'AND') {
        // AND logic: Create sequential filter steps (current behavior)
        filterGroup.conditions.forEach((condition, idx) => {
          // Skip no-op conditions that would always pass
          if (isNoOpCondition(condition)) {
            this.log(ctx, `⚠️ Skipping no-op filter: ${condition.field} ${condition.operator} "${condition.value}" (would always pass)`)
            return
          }

          const metadata = this.generateStepMetadata(`filter_${condition.field}`, `Filter ${condition.field}`, ctx)
          steps.push({
            ...metadata,
            type: 'transform',
            operation: 'filter',
            input: `{{${ctx.currentVariable}}}`,
            config: {
              condition: {
                conditionType: 'simple',  // CRITICAL FIX: Required discriminator for ConditionalEvaluator
                field: condition.field,
                operator: condition.operator,
                value: condition.value
              }
            }
          })

          // Filter transform outputs are stored in .data
          ctx.currentVariable = `${metadata.id}.data`
          this.log(ctx, `✓ Compiled filter (AND): ${condition.field} ${condition.operator} ${condition.value}`)
        })
      } else {
        // OR logic: Create single filter step with complex OR condition
        // Filter out no-op conditions first
        const validConditions = filterGroup.conditions.filter(c => !isNoOpCondition(c))

        if (validConditions.length === 0) {
          this.log(ctx, `⚠️ Skipping entire OR filter: all conditions are no-ops`)
        } else {
          const metadata = this.generateStepMetadata('filter_or', 'Filter OR', ctx)
          steps.push({
            ...metadata,
            type: 'transform',
            operation: 'filter',
            input: `{{${ctx.currentVariable}}}`,
            config: {
              condition: {
                conditionType: 'complex_or',  // CRITICAL FIX: Use proper complex condition type
                conditions: validConditions.map(c => ({
                  conditionType: 'simple',
                  field: c.field,
                  operator: c.operator,
                  value: c.value
                }))
              }
            }
          })

          // Filter transform outputs are stored in .data
          ctx.currentVariable = `${metadata.id}.data`
          this.log(ctx, `✓ Compiled filter (OR): ${validConditions.length} conditions`)
        }
      }
    }

    // Handle complex nested groups (e.g., (A AND B) OR (C AND D))
    if (filterGroup.groups && filterGroup.groups.length > 0) {
      filterGroup.groups.forEach((group, groupIdx) => {
        // Filter out no-op conditions from each group
        const validConditions = group.conditions.filter(c => !isNoOpCondition(c))

        if (validConditions.length === 0) {
          this.log(ctx, `⚠️ Skipping filter group ${groupIdx + 1}: all conditions are no-ops`)
          return
        }

        const metadata = this.generateStepMetadata(`filter_group_${groupIdx + 1}`, `Filter Group ${groupIdx + 1}`, ctx)
        const conditionType = group.combineWith === 'AND' ? 'complex_and' : 'complex_or'
        steps.push({
          ...metadata,
          type: 'transform',
          operation: 'filter',
          input: `{{${ctx.currentVariable}}}`,
          config: {
            condition: {
              conditionType,  // CRITICAL FIX: Use proper complex condition type discriminator
              conditions: validConditions.map(c => ({
                conditionType: 'simple',
                field: c.field,
                operator: c.operator,
                value: c.value
              }))
            }
          }
        })

        // Filter transform outputs are stored in .data
        ctx.currentVariable = `${metadata.id}.data`
        this.log(ctx, `✓ Compiled filter group (${group.combineWith}): ${group.conditions.length} conditions`)
      })
    }

    return steps
  }

  // ==========================================================================
  // Post-AI Filters Compilation (applied after AI operations)
  // ==========================================================================

  /**
   * Compile post-AI filters - filters applied AFTER AI operations on AI output fields.
   * These are used to filter results based on AI-generated classifications/extractions.
   * Example: Only show emails where action_required = true
   *
   * @param ir - The declarative IR
   * @param ctx - The compiler context with current variable tracking
   * @returns The generated workflow steps
   */
  private compilePostAIFilters(ir: DeclarativeLogicalIR, ctx: CompilerContext): WorkflowStep[] {
    const steps: WorkflowStep[] = []

    if (!ir.post_ai_filters) {
      return steps
    }

    const filterGroup = ir.post_ai_filters
    this.log(ctx, 'Compiling post-AI filters...')

    // Handle simple conditions with AND/OR logic
    if (filterGroup.conditions && filterGroup.conditions.length > 0) {
      const combineWith = filterGroup.combineWith || 'AND'

      if (combineWith === 'AND') {
        // AND logic: Create sequential filter steps
        filterGroup.conditions.forEach((condition) => {
          const metadata = this.generateStepMetadata(`post_ai_filter_${condition.field}`, `Post-AI Filter: ${condition.field}`, ctx)
          steps.push({
            ...metadata,
            type: 'transform',
            operation: 'filter',
            input: `{{${ctx.currentVariable}}}`,
            config: {
              condition: {
                conditionType: 'simple',
                field: condition.field,
                operator: condition.operator,
                value: condition.value
              }
            }
          })

          ctx.currentVariable = `${metadata.id}.data`
          this.log(ctx, `✓ Compiled post-AI filter (AND): ${condition.field} ${condition.operator} ${condition.value}`)
        })
      } else {
        // OR logic: Create single filter step with complex OR condition
        const metadata = this.generateStepMetadata('post_ai_filter_or', 'Post-AI Filter OR', ctx)
        steps.push({
          ...metadata,
          type: 'transform',
          operation: 'filter',
          input: `{{${ctx.currentVariable}}}`,
          config: {
            condition: {
              conditionType: 'complex_or',
              conditions: filterGroup.conditions.map(c => ({
                conditionType: 'simple',
                field: c.field,
                operator: c.operator,
                value: c.value
              }))
            }
          }
        })

        ctx.currentVariable = `${metadata.id}.data`
        this.log(ctx, `✓ Compiled post-AI filter (OR): ${filterGroup.conditions.length} conditions`)
      }
    }

    // Handle complex nested groups
    if (filterGroup.groups && filterGroup.groups.length > 0) {
      filterGroup.groups.forEach((group, groupIdx) => {
        const metadata = this.generateStepMetadata(`post_ai_filter_group_${groupIdx + 1}`, `Post-AI Filter Group ${groupIdx + 1}`, ctx)
        const conditionType = group.combineWith === 'AND' ? 'complex_and' : 'complex_or'
        steps.push({
          ...metadata,
          type: 'transform',
          operation: 'filter',
          input: `{{${ctx.currentVariable}}}`,
          config: {
            condition: {
              conditionType: conditionType,
              conditions: group.conditions.map(c => ({
                conditionType: 'simple',
                field: c.field,
                operator: c.operator,
                value: c.value
              }))
            }
          }
        })

        ctx.currentVariable = `${metadata.id}.data`
        this.log(ctx, `✓ Compiled post-AI filter group ${groupIdx + 1} (${group.combineWith}): ${group.conditions.length} conditions`)
      })
    }

    return steps
  }

  // ==========================================================================
  // Sorting Compilation (applied before rendering/delivery)
  // ==========================================================================

  /**
   * Compile sorting specification - applied before rendering to order output data.
   * Supports multi-level sorting with priority (primary sort, secondary sort, etc.)
   * Example: Sort by Priority (desc), then by Received time (desc)
   *
   * @param ir - The declarative IR
   * @param ctx - The compiler context with current variable tracking
   * @returns The generated workflow steps
   */
  private compileSorting(ir: DeclarativeLogicalIR, ctx: CompilerContext): WorkflowStep[] {
    const steps: WorkflowStep[] = []

    if (!ir.rendering?.sort_order || ir.rendering.sort_order.length === 0) {
      return steps
    }

    const sortSpecs = ir.rendering.sort_order
    this.log(ctx, `Compiling sorting with ${sortSpecs.length} sort field(s)...`)

    // Sort by priority if specified, otherwise use array order
    const sortedSpecs = [...sortSpecs].sort((a, b) => {
      const priorityA = a.priority ?? 999
      const priorityB = b.priority ?? 999
      return priorityA - priorityB
    })

    // Create a single sort step with all fields (multi-level sort)
    const metadata = this.generateStepMetadata('sort_data', 'Sort Data', ctx)

    // Build sort configuration
    // For multi-level sort, we pass an array of sort criteria
    const sortConfig: any = {
      sort_by: sortedSpecs.map(spec => ({
        field: spec.field,
        direction: spec.direction || 'asc'
      }))
    }

    // If only one sort field, simplify the config
    if (sortedSpecs.length === 1) {
      sortConfig.sort_by = sortedSpecs[0].field
      sortConfig.order = sortedSpecs[0].direction || 'asc'
    }

    steps.push({
      ...metadata,
      type: 'transform',
      operation: 'sort',
      input: `{{${ctx.currentVariable}}}`,
      config: sortConfig
    })

    ctx.currentVariable = `${metadata.id}.data`

    const sortDescription = sortedSpecs.map(s => `${s.field} (${s.direction || 'asc'})`).join(', then by ')
    this.log(ctx, `✓ Compiled sorting: ${sortDescription}`)

    return steps
  }

  // ==========================================================================
  // Shared AI Operations Processing (used by all delivery patterns)
  // ==========================================================================

  /**
   * Process AI operations and add appropriate steps to the workflow.
   * This is shared logic used by compileSummaryDelivery, compileMultiDestinationDelivery, etc.
   *
   * Handles:
   * - File-based extraction ops (with attachment flattening and scatter-gather)
   * - Summarization ops
   * - Semantic ops (classify, transform, extract, generate, etc.)
   *
   * @returns The steps to add and the updated current variable
   */
  private processAIOperationsForDelivery(
    ir: DeclarativeLogicalIR,
    ctx: CompilerContext
  ): { steps: WorkflowStep[], currentVariable: string } {
    const steps: WorkflowStep[] = []

    if (!ir.ai_operations || ir.ai_operations.length === 0) {
      return { steps, currentVariable: ctx.currentVariable }
    }

    this.log(ctx, `Processing ${ir.ai_operations.length} AI operations`)

    let extractedDataVariable = ctx.currentVariable

    // Helper to detect file-based extraction
    const isFileBasedExtraction = (op: any): boolean => {
      const context = this.getContextString(op.context).toLowerCase()
      const instruction = (op.instruction || '').toLowerCase()
      return (
        op.type === 'deterministic_extract' ||
        context.includes('attachment') ||
        context.includes('pdf') ||
        context.includes('image') ||
        context.includes('file') ||
        instruction.includes('attachment') ||
        instruction.includes('pdf') ||
        instruction.includes('from the file') ||
        instruction.includes('from the document')
      )
    }

    // STEP 1: Process file-based extraction operations
    const extractionOps = ir.ai_operations.filter(isFileBasedExtraction)

    if (extractionOps.length > 0) {
      this.log(ctx, `Detected ${extractionOps.length} extraction operations requiring iteration`)

      const firstOp = extractionOps[0]
      const itemVariable = 'item'

      // Auto-inject attachment extraction
      const extractMetadata = this.generateStepMetadata('extract_attachments', 'Extract Attachments', ctx)
      steps.push({
        ...extractMetadata,
        type: 'transform',
        operation: 'flatten',
        input: `{{${ctx.currentVariable}}}`,
        config: { field: 'attachments' }
      })

      ctx.currentVariable = `${extractMetadata.id}.data`
      extractedDataVariable = ctx.currentVariable
      this.log(ctx, '✓ Auto-injected attachment extraction transform')

      // Build steps for scatter-gather
      const scatterSteps: any[] = []

      // Detect if we need to fetch file content
      const needsContentFetch = this.detectNeedsContentFetch(
        this.getContextString(firstOp.context) || 'attachments',
        firstOp
      )

      if (needsContentFetch) {
        const primaryDataSource = ir.data_sources?.[0]
        const sourcePlugin = primaryDataSource?.plugin_key

        if (!sourcePlugin) {
          throw new Error(`data_source.plugin_key is required for fetch_content: ${primaryDataSource?.source}`)
        }

        const fetchContentMetadata = this.generateStepMetadata('fetch_content', 'Fetch File Content', ctx)
        scatterSteps.push({
          ...fetchContentMetadata,
          type: 'transform',
          operation: 'fetch_content',
          input: `{{${itemVariable}}}`,
          config: { source_plugin: sourcePlugin },
          output_variable: 'file_with_content'
        })
        this.log(ctx, `✓ Auto-injected fetch_content step for ${sourcePlugin} files`)
      }

      // Create AI extraction step
      const aiInputVariable = needsContentFetch ? 'file_with_content' : itemVariable
      const extractionStep = this.compileAIOperation(firstOp, aiInputVariable, ctx)
      scatterSteps.push(extractionStep)

      const loopExtractionMetadata = this.generateStepMetadata('loop_items', 'Loop AI Extraction', ctx)
      steps.push({
        ...loopExtractionMetadata,
        type: 'scatter_gather',
        scatter: {
          input: `{{${ctx.currentVariable}}}`,
          itemVariable: itemVariable,
          steps: scatterSteps
        },
        gather: { operation: 'collect' },
        output_variable: 'ai_extraction_results'
      })

      ctx.currentVariable = 'ai_extraction_results'
      extractedDataVariable = ctx.currentVariable
      this.log(ctx, '✓ Created scatter-gather loop with AI extraction')

      // ✅ FIX: Capture AI output fields for column mapping in rendering
      // For deterministic_extract, fields come from output_schema.fields (object type) or output_schema.items.fields (array type)
      const outputSchema = firstOp.output_schema
      const aiOutputFields: string[] = []
      if (outputSchema) {
        const fieldsArray = outputSchema.type === 'array'
          ? outputSchema.items?.fields
          : outputSchema.fields
        if (fieldsArray && Array.isArray(fieldsArray)) {
          for (const field of fieldsArray) {
            if (field.name && !aiOutputFields.includes(field.name)) {
              aiOutputFields.push(field.name)
            }
          }
        }
      }
      if (aiOutputFields.length > 0) {
        ctx.aiOutputFields = aiOutputFields
        this.log(ctx, `✓ Captured ${aiOutputFields.length} extraction output fields for column mapping: ${aiOutputFields.join(', ')}`)
      }

      // Extract actual data from AI results (outputSchema already captured above)
      // The scatter-gather returns an array of extraction results, where each result has:
      // { filename: "...", data: [...extracted items...], confidence: ..., metadata: ... }
      //
      // We ALWAYS need to extract the 'data' field and flatten it, regardless of output_schema.type
      // - For 'object' type: each result.data is a single object → flatten gives array of objects
      // - For 'array' type: each result.data is an array → flatten gives flat array of all items
      if (outputSchema) {
        this.log(ctx, `✓ AI output schema type: ${outputSchema.type} - will extract and flatten 'data' field from extraction results`)

        const flattenMetadata = this.generateStepMetadata('flatten_results', 'Flatten AI Results', ctx)
        steps.push({
          ...flattenMetadata,
          type: 'transform',
          operation: 'flatten',
          input: `{{${ctx.currentVariable}}}`,
          config: { field: 'data' }
        })

        ctx.currentVariable = `${flattenMetadata.id}.data`
        extractedDataVariable = ctx.currentVariable
        this.log(ctx, '✓ Added flatten transform to extract data field from AI extraction results')
      }
    }

    // STEP 2: Process summarization operations
    const summarizeOps = ir.ai_operations.filter(op => op.type === 'summarize')
    if (summarizeOps.length > 0) {
      this.log(ctx, `Processing ${summarizeOps.length} summarization operations`)

      summarizeOps.forEach((summarizeOp) => {
        const summarizeStep = this.compileAIOperation(summarizeOp, extractedDataVariable, ctx)
        steps.push({
          step_id: summarizeStep.step_id,
          type: summarizeStep.type,
          config: {
            ...summarizeStep.config,
            input: `{{${extractedDataVariable}}}`
          },
          output_variable: summarizeStep.output_variable
        })

        this.log(ctx, `✓ Added summarization AI operation: ${summarizeOp.type}`)
      })
    }

    // STEP 3: Process semantic AI operations (classify, transform, extract, generate, etc.)
    const semanticOps = ir.ai_operations.filter(op =>
      op.type === 'transform' || op.type === 'classify' || op.type === 'generate' ||
      op.type === 'sentiment' || op.type === 'enrich' || op.type === 'normalize' ||
      op.type === 'validate' || op.type === 'decide' || op.type === 'extract'
    ).filter(op => !isFileBasedExtraction(op)) // Exclude file-based ops (already handled)

    if (semanticOps.length > 0) {
      this.log(ctx, `Processing ${semanticOps.length} semantic AI operations`)

      // Check if primary data source returns a single result object
      const primaryPlugin = ctx.primaryDataSourcePlugin || ''
      const isSingleResultSource = this.pluginReturnsSingleResultObject(primaryPlugin)

      if (isSingleResultSource) {
        this.log(ctx, `✓ Primary data source (${primaryPlugin}) returns single result - forcing batch processing`)
      }

      // Helper: Detect if operation should be per-item or batch
      // Uses operation TYPE as primary signal - this is semantically correct:
      // - classify/extract/transform on items → per-item processing
      // - summarize/aggregate → batch processing (combines multiple items)
      const needsPerItemProcessing = (op: any): boolean => {
        if (isSingleResultSource) return false

        const opType = (op.type || '').toLowerCase()
        const instruction = (op.instruction || '').toLowerCase()
        const context = this.getContextString(op.context).toLowerCase()

        // Batch operation types - these aggregate/combine multiple items
        const batchOperationTypes = ['summarize', 'aggregate', 'combine', 'merge', 'reduce']
        if (batchOperationTypes.includes(opType)) {
          return false
        }

        // Per-item operation types - these process individual items
        const perItemOperationTypes = ['classify', 'extract', 'transform', 'generate', 'enrich', 'validate']
        if (perItemOperationTypes.includes(opType)) {
          // Double-check: if instruction explicitly mentions aggregation, treat as batch
          const batchKeywords = ['all items', 'summarize all', 'combine all', 'aggregate']
          for (const keyword of batchKeywords) {
            if (instruction.includes(keyword)) {
              return false
            }
          }
          return true
        }

        // Fallback: check for explicit per-item keywords in instruction/context
        const perItemKeywords = [
          'each email', 'each item', 'each record', 'each row',
          'per email', 'per item', 'per record', 'every email', 'every item',
          'individual email', 'individually', 'for each', 'each thread', 'per thread',
          'each message', 'per message'
        ]

        for (const keyword of perItemKeywords) {
          if (instruction.includes(keyword) || context.includes(keyword)) {
            return true
          }
        }
        return false
      }

      const perItemOps = semanticOps.filter(needsPerItemProcessing)
      const batchOps = semanticOps.filter(op => !needsPerItemProcessing(op))

      // CRITICAL FIX: If we have per-item ops AND batch ops, include batch ops inside the scatter-gather
      // This ensures all AI operations produce per-item results that can be mapped downstream
      // Without this fix, batch ops would receive an array but produce a single object, breaking map operations
      const hasPerItemOps = perItemOps.length > 0
      const hasBatchOps = batchOps.length > 0

      if (hasPerItemOps) {
        this.log(ctx, `Processing ${perItemOps.length} per-item + ${batchOps.length} batch semantic AI operations (all in scatter-gather)`)

        const scatterSteps: any[] = []

        // First, add all per-item operations
        perItemOps.forEach((semanticOp) => {
          const aiStep = this.compileAIOperation(semanticOp, 'item', ctx)
          scatterSteps.push(aiStep)
          this.log(ctx, `✓ Added per-item AI operation to scatter: ${semanticOp.type}`)
        })

        // Then, chain batch operations INSIDE the scatter-gather
        // This ensures they process each item individually, not the whole array
        if (hasBatchOps) {
          this.log(ctx, `Including ${batchOps.length} batch ops inside scatter-gather to maintain per-item flow`)

          // Batch ops will be chained after per-item ops
          // First batch op takes output from last per-item op
          let chainedInputVariable = perItemOps.length > 0
            ? `${scatterSteps[scatterSteps.length - 1].id}.data`
            : 'item'

          batchOps.forEach((semanticOp, idx) => {
            const aiStep = this.compileAIOperation(semanticOp, chainedInputVariable, ctx)
            scatterSteps.push(aiStep)
            chainedInputVariable = `${aiStep.id}.data`
            this.log(ctx, `✓ Added chained batch AI operation ${idx + 1}/${batchOps.length} to scatter: ${semanticOp.type}`)
          })
        }

        const loopSemanticMetadata = this.generateStepMetadata('loop_semantic_ai', 'Loop Semantic AI Processing', ctx)
        steps.push({
          ...loopSemanticMetadata,
          type: 'scatter_gather',
          scatter: {
            input: `{{${extractedDataVariable}}}`,
            itemVariable: 'item',
            steps: scatterSteps
          },
          gather: { operation: 'collect' },
          output_variable: 'ai_semantic_results'
        })

        ctx.currentVariable = 'ai_semantic_results'
        this.log(ctx, `✓ Created scatter-gather loop for all semantic AI processing (${scatterSteps.length} steps)`)

        // ✅ FIX: Capture AI output fields for column mapping
        // The scatter-gather produces merged results: original item fields + AI output fields
        // We need to track these AI fields so they can be included in the rendering columns
        const aiOutputFields: string[] = []
        for (const op of [...perItemOps, ...batchOps]) {
          if (op.output_schema?.fields) {
            for (const field of op.output_schema.fields) {
              if (field.name && !aiOutputFields.includes(field.name)) {
                aiOutputFields.push(field.name)
              }
            }
          }
        }
        if (aiOutputFields.length > 0) {
          ctx.aiOutputFields = aiOutputFields
          this.log(ctx, `✓ Captured ${aiOutputFields.length} AI output fields for column mapping: ${aiOutputFields.join(', ')}`)
        }
      } else if (hasBatchOps) {
        // No per-item ops, only batch ops
        // Process batch operations - chain them so each gets output of previous
        this.log(ctx, `Processing ${batchOps.length} batch semantic AI operations (chained, no scatter-gather)`)

        let chainedInputVariable = extractedDataVariable

        batchOps.forEach((semanticOp, idx) => {
          // Each batch op takes input from the previous op's output
          const aiStep = this.compileAIOperation(semanticOp, chainedInputVariable, ctx)
          steps.push(aiStep)

          // Update for next operation in chain
          // Use stepId.data pattern which is universally supported by the execution engine
          chainedInputVariable = `${aiStep.id}.data`
          ctx.currentVariable = `${aiStep.id}.data`

          this.log(ctx, `✓ Added batch AI operation ${idx + 1}/${batchOps.length}: ${semanticOp.type} (output: ${ctx.currentVariable})`)
        })
      }
    }

    return { steps, currentVariable: ctx.currentVariable }
  }

  // ==========================================================================
  // Delivery Pattern Compilation (LOOP INFERENCE!)
  // ==========================================================================

  private compileDeliveryPattern(ir: DeclarativeLogicalIR, ctx: CompilerContext): WorkflowStep[] {
    const { delivery_rules } = ir
    const steps: WorkflowStep[] = []

    // Pattern 0: Multi-Destination Delivery (can coexist with other patterns)
    // If multiple_destinations is specified, send to all destinations in parallel
    if (delivery_rules.multiple_destinations && delivery_rules.multiple_destinations.length > 0) {
      this.log(ctx, `Detected pattern: Multi-Destination Delivery → Will send to ${delivery_rules.multiple_destinations.length} destinations in parallel`)
      return this.compileMultiDestinationDelivery(ir, ctx)
    }

    // Pattern 1: Per-Group Delivery
    if (delivery_rules.per_group_delivery) {
      this.log(ctx, 'Detected pattern: Per-Group Delivery → Will create partition + group + loop')
      return this.compilePerGroupDelivery(ir, ctx)
    }

    // Pattern 2: Per-Item Delivery
    if (delivery_rules.per_item_delivery) {
      this.log(ctx, 'Detected pattern: Per-Item Delivery → Will create loop over items')
      return this.compilePerItemDelivery(ir, ctx)
    }

    // Pattern 3: Summary Delivery
    if (delivery_rules.summary_delivery) {
      this.log(ctx, 'Detected pattern: Summary Delivery → Single delivery, no loop')
      return this.compileSummaryDelivery(ir, ctx)
    }

    return steps
  }

  // ==========================================================================
  // Per-Group Delivery Pattern
  // ==========================================================================

  private compilePerGroupDelivery(ir: DeclarativeLogicalIR, ctx: CompilerContext): WorkflowStep[] {
    const steps: WorkflowStep[] = []
    const { per_group_delivery } = ir.delivery_rules
    const grouping = ir.grouping!

    // Apply post-AI filters before partitioning (to filter items before grouping)
    if (ir.post_ai_filters && (ir.post_ai_filters.conditions || ir.post_ai_filters.groups)) {
      const postAIFilterSteps = this.compilePostAIFilters(ir, ctx)
      steps.push(...postAIFilterSteps)
    }

    // Apply sorting before partitioning (items in each group will be sorted)
    if (ir.rendering?.sort_order && ir.rendering.sort_order.length > 0) {
      const sortSteps = this.compileSorting(ir, ctx)
      steps.push(...sortSteps)
    }

    // Step 1: Partition data by group field
    if (ir.partitions && ir.partitions.length > 0) {
      const partition = ir.partitions.find(p => p.field === grouping.group_by)
      if (partition) {
        const metadata = this.generateStepMetadata('partition', 'Partition Data', ctx)
        steps.push({
          ...metadata,
          type: 'transform',
          operation: 'partition',
          input: `{{${ctx.currentVariable}}}`,
          config: {
            field: partition.field,
            handle_empty: partition.handle_empty
          }
        })

        ctx.currentVariable = `${metadata.id}.assigned`
        this.log(ctx, `✓ Created partition by ${partition.field}`)
      }
    }

    // Step 2: Group data
    const groupMetadata = this.generateStepMetadata('group_by', 'Group By Field', ctx)
    steps.push({
      ...groupMetadata,
      type: 'transform',
      operation: 'group_by',
      input: `{{${ctx.currentVariable}}}`,
      config: {
        field: grouping.group_by
      }
    })

    // Transform step outputs are stored in .data
    ctx.currentVariable = `${groupMetadata.id}.data`
    this.log(ctx, `✓ Created grouping by ${grouping.group_by}`)

    // Step 3: Create scatter-gather loop over groups
    const loopActions: any[] = []

    // If there's rendering, add it
    if (ir.rendering) {
      const renderMetadata = this.generateStepMetadata('render_group', 'Render Group Table', ctx)

      // Map IR column names to actual plugin output field names
      let mappedColumns = ir.rendering.columns_in_order
      if (ctx.primaryDataSourcePlugin && ctx.primaryDataSourceOperation && mappedColumns) {
        mappedColumns = this.mapColumnsToPluginFields(
          mappedColumns,
          ctx.primaryDataSourcePlugin,
          ctx.primaryDataSourceOperation,
          ctx
        )
      }

      // ✅ FIX: When AI extraction is the primary data source, USE AI output fields ONLY
      // The IR columns_in_order represent user-specified names, but after extraction,
      // the actual data fields are from the extraction output_schema
      if (ctx.aiOutputFields && ctx.aiOutputFields.length > 0) {
        const hasDeterministicExtraction = ir.ai_operations?.some(op => op.type === 'deterministic_extract')

        if (hasDeterministicExtraction) {
          // Replace columns entirely with AI output fields - the IR columns are stale
          this.log(ctx, `✓ Deterministic extraction detected - using AI output fields as columns: ${ctx.aiOutputFields.join(', ')}`)
          mappedColumns = [...ctx.aiOutputFields]
        } else {
          // For other AI operations (classify, transform), append AI fields to existing columns
          const existingColumns = new Set((mappedColumns || []).map(c => c.toLowerCase()))
          const newAIFields = ctx.aiOutputFields.filter(f => !existingColumns.has(f.toLowerCase()))

          if (newAIFields.length > 0) {
            mappedColumns = [...(mappedColumns || []), ...newAIFields]
            this.log(ctx, `✓ Added ${newAIFields.length} AI output fields to columns: ${newAIFields.join(', ')}`)
          }
        }
      }

      loopActions.push({
        ...renderMetadata,
        type: 'transform',
        operation: 'render_table',
        input: `{{group.items}}`,
        config: {
          rendering_type: ir.rendering.type,
          columns: mappedColumns,
          empty_message: ir.rendering.empty_message
        }
      })

      this.log(ctx, `✓ Added rendering step in loop`)
    }

    // Add delivery via plugin (email, slack, etc.)
    if (!per_group_delivery!.plugin_key) {
      throw new Error('per_group_delivery.plugin_key is required for compilation')
    }

    const groupDeliveryPluginKey = per_group_delivery!.plugin_key
    const groupDeliveryOpType = per_group_delivery!.operation_type || 'send'

    // Resolve delivery plugin with error handling
    let groupDeliveryResolution
    try {
      groupDeliveryResolution = this.pluginResolver.resolveDelivery(groupDeliveryPluginKey, groupDeliveryOpType)
    } catch (error) {
      const errorMsg = `Failed to resolve per-group delivery plugin: ${groupDeliveryPluginKey}.${groupDeliveryOpType}`
      this.log(ctx, `✗ ${errorMsg}`)
      throw new Error(`${errorMsg}: ${error instanceof Error ? error.message : String(error)}`)
    }

    // Build delivery params using plugin-agnostic mapping
    const groupDeliveryParams = this.buildDeliveryParams(
      per_group_delivery!,
      groupDeliveryResolution.parameters_schema,
      ctx,
      ir.rendering ? 'rendered_table' : 'group.items'
    )

    // Override recipient with group key for per-group delivery
    if (groupDeliveryParams.to || groupDeliveryParams.recipients) {
      groupDeliveryParams.to = [`{{group.key}}`]
    }

    const groupSendMetadata = this.generateStepMetadata('send_group', `Send to Group via ${groupDeliveryPluginKey}`, ctx)
    loopActions.push({
      ...groupSendMetadata,
      type: 'action',
      plugin: groupDeliveryResolution.plugin_name,
      action: groupDeliveryResolution.operation,  // Use 'action' for PILOT executor compatibility
      params: groupDeliveryParams
    })

    this.log(ctx, `✓ Added group delivery via ${groupDeliveryResolution.plugin_name}.${groupDeliveryResolution.operation}`)

    // Create the scatter-gather step
    const loopGroupsMetadata = this.generateStepMetadata('loop_groups', 'Loop over Groups', ctx)
    steps.push({
      ...loopGroupsMetadata,
      type: 'scatter_gather',
      scatter: {
        input: `{{${ctx.currentVariable}}}`,
        itemVariable: 'group',
        steps: loopActions
      },
      gather: { operation: 'collect' },
      output_variable: 'delivery_results'
    })

    this.log(ctx, `✓ Created scatter-gather loop over groups`)

    return steps
  }

  // ==========================================================================
  // Per-Item Delivery Pattern
  // ==========================================================================

  private compilePerItemDelivery(ir: DeclarativeLogicalIR, ctx: CompilerContext): WorkflowStep[] {
    const steps: WorkflowStep[] = []
    const { per_item_delivery } = ir.delivery_rules

    // Apply post-AI filters before the loop (to filter items before delivery)
    // Note: For per-item delivery, AI ops are inside the loop, but post_ai_filters
    // typically apply to batch-processed data. If post_ai_filters exist here,
    // they likely filter on source fields, not AI output fields.
    if (ir.post_ai_filters && (ir.post_ai_filters.conditions || ir.post_ai_filters.groups)) {
      const postAIFilterSteps = this.compilePostAIFilters(ir, ctx)
      steps.push(...postAIFilterSteps)
    }

    // Apply sorting before the loop (to process items in order)
    if (ir.rendering?.sort_order && ir.rendering.sort_order.length > 0) {
      const sortSteps = this.compileSorting(ir, ctx)
      steps.push(...sortSteps)
    }

    // If there are AI operations, inject them into the loop
    const loopActions: any[] = []

    // Add AI operations if present
    if (ir.ai_operations && ir.ai_operations.length > 0) {
      ir.ai_operations.forEach((aiOp) => {
        loopActions.push(this.compileAIOperation(aiOp, 'item', ctx))
        this.log(ctx, `✓ Added AI operation to loop: ${aiOp.type}`)
      })
    }

    // Add delivery via plugin (email, slack, etc.)
    if (!per_item_delivery!.plugin_key) {
      throw new Error('per_item_delivery.plugin_key is required for compilation')
    }

    const itemDeliveryPluginKey = per_item_delivery!.plugin_key
    const itemDeliveryOpType = per_item_delivery!.operation_type || 'send'

    // Resolve delivery plugin with error handling
    let itemDeliveryResolution
    try {
      itemDeliveryResolution = this.pluginResolver.resolveDelivery(itemDeliveryPluginKey, itemDeliveryOpType)
    } catch (error) {
      const errorMsg = `Failed to resolve per-item delivery plugin: ${itemDeliveryPluginKey}.${itemDeliveryOpType}`
      this.log(ctx, `✗ ${errorMsg}`)
      throw new Error(`${errorMsg}: ${error instanceof Error ? error.message : String(error)}`)
    }

    // Build delivery params using plugin-agnostic mapping
    const itemDeliveryParams = this.buildDeliveryParams(
      per_item_delivery!,
      itemDeliveryResolution.parameters_schema,
      ctx,
      per_item_delivery!.body_template || 'item'
    )

    // Override recipient with item field for per-item delivery
    if (itemDeliveryParams.to || itemDeliveryParams.recipients) {
      itemDeliveryParams.to = [`{{item.${per_item_delivery!.recipient_source}}}`]
    }

    const itemSendMetadata = this.generateStepMetadata('send_item', `Send per Item via ${itemDeliveryPluginKey}`, ctx)
    loopActions.push({
      ...itemSendMetadata,
      type: 'action',
      plugin: itemDeliveryResolution.plugin_name,
      action: itemDeliveryResolution.operation,  // Use 'action' for PILOT executor compatibility
      params: itemDeliveryParams
    })

    this.log(ctx, `✓ Added item delivery via ${itemDeliveryResolution.plugin_name}.${itemDeliveryResolution.operation}`)

    // Create scatter-gather loop
    const loopItemsMetadata = this.generateStepMetadata('loop_items', 'Loop over Items', ctx)
    steps.push({
      ...loopItemsMetadata,
      type: 'scatter_gather',
      scatter: {
        input: `{{${ctx.currentVariable}}}`,
        itemVariable: 'item',
        steps: loopActions
      },
      gather: { operation: 'collect' },
      output_variable: 'delivery_results'
    })

    this.log(ctx, `✓ Created scatter-gather loop over items`)

    return steps
  }

  // ==========================================================================
  // Summary Delivery Pattern
  // ==========================================================================

  private compileSummaryDelivery(ir: DeclarativeLogicalIR, ctx: CompilerContext): WorkflowStep[] {
    const steps: WorkflowStep[] = []
    const { summary_delivery } = ir.delivery_rules

    // Check if we need to process with AI first
    if (ir.ai_operations && ir.ai_operations.length > 0) {
      this.log(ctx, `Processing ${ir.ai_operations.length} AI operations`)

      // Initialize extractedDataVariable to current context variable
      // This ensures semantic ops work even if there are no extraction ops
      let extractedDataVariable = ctx.currentVariable

      // For API sources with AI operations, we need to loop over items first
      // Example: Loop over attachments/files, extract data, then summarize

      // STEP 1: Process extraction AI operations that specifically require file/attachment content
      // Only flatten attachments if the AI operation explicitly works with files (pdf, image, docx, etc.)
      // NOT for operations that just process email body/snippet text
      const extractionOps = ir.ai_operations.filter(op => {
        const context = this.getContextString(op.context).toLowerCase()
        const instruction = (op.instruction || '').toLowerCase()

        // Check if this is a file-based extraction (needs attachment content)
        const isFileBasedExtraction =
          op.type === 'deterministic_extract' ||
          context.includes('attachment') ||
          context.includes('pdf') ||
          context.includes('image') ||
          context.includes('file') ||
          instruction.includes('attachment') ||
          instruction.includes('pdf') ||
          instruction.includes('from the file') ||
          instruction.includes('from the document')

        // Regular 'extract' type alone is NOT enough - it could be extracting from email body
        // Only include if there's explicit file/attachment mention
        return isFileBasedExtraction
      })

      if (extractionOps.length > 0) {
        this.log(ctx, `Detected ${extractionOps.length} extraction operations requiring iteration`)

        const firstOp = extractionOps[0]
        const context = this.getContextString(firstOp.context) || 'attachments'

        // Use generic 'item' as variable name - fully agnostic!
        const itemVariable = 'item'
        const description = `Extract ${context}`

        // Dynamically extract file type requirements from context/instruction
        // This works for ANY file type mentioned (pdf, docx, xlsx, png, etc.)
        const fileTypeFilter = this.extractFileTypeFilter(firstOp)

        // Auto-inject attachment extraction transform
        const extractMetadata = this.generateStepMetadata('extract_attachments', description, ctx)
        const extractConfig: any = {
          field: 'attachments',
          flatten: true
        }

        // Add file type filter if detected (plugin-agnostic!)
        if (fileTypeFilter) {
          extractConfig.filter = fileTypeFilter
          this.log(ctx, `✓ Detected file type filter from AI operation: ${JSON.stringify(fileTypeFilter)}`)
        }

        steps.push({
          ...extractMetadata,
          type: 'transform',
          operation: 'flatten',
          input: `{{${ctx.currentVariable}}}`,
          config: {
            field: 'attachments'
          }
        })

        // Transform step outputs are stored in .data
        ctx.currentVariable = `${extractMetadata.id}.data`
        this.log(ctx, `✓ Auto-injected attachment extraction transform (${context})`)

        // Build steps for scatter-gather
        const scatterSteps: any[] = []

        // ✨ SMART COMPILER: Detect if we need to fetch file content before AI extraction
        // Attachments from APIs only contain metadata (filename, id, mimeType)
        // We need to fetch actual content for AI to analyze
        const needsContentFetch = this.detectNeedsContentFetch(context, firstOp)

        if (needsContentFetch) {
          // Use plugin_key from data source (required field per IR validation)
          const primaryDataSource = ir.data_sources?.[0]
          const sourcePlugin = primaryDataSource?.plugin_key

          if (!sourcePlugin) {
            throw new Error(`data_source.plugin_key is required for fetch_content: ${primaryDataSource?.source}`)
          }

          const fetchContentMetadata = this.generateStepMetadata('fetch_content', 'Fetch File Content', ctx)
          const fetchContentStep = {
            ...fetchContentMetadata,
            type: 'transform',
            operation: 'fetch_content',
            input: `{{${itemVariable}}}`,
            config: {
              source_plugin: sourcePlugin
            },
            output_variable: 'file_with_content'
          }
          scatterSteps.push(fetchContentStep)
          this.log(ctx, `✓ Auto-injected fetch_content step for ${sourcePlugin} files`)
        }

        // Create loop over items with extraction AI operation
        // If we fetched content, the AI step should use that; otherwise use original item
        const aiInputVariable = needsContentFetch ? 'file_with_content' : itemVariable
        const extractionStep = this.compileAIOperation(firstOp, aiInputVariable, ctx)
        scatterSteps.push(extractionStep)

        const loopExtractionMetadata = this.generateStepMetadata('loop_items', `Loop AI Extraction (${context})`, ctx)
        steps.push({
          ...loopExtractionMetadata,
          type: 'scatter_gather',
          scatter: {
            input: `{{${ctx.currentVariable}}}`,
            itemVariable: itemVariable,
            steps: scatterSteps
          },
          gather: { operation: 'collect' },
          output_variable: 'ai_extraction_results'
        })

        ctx.currentVariable = 'ai_extraction_results'
        this.log(ctx, `✓ Created scatter-gather loop with AI extraction (${context})`)

        // ✅ FIX: Capture AI output fields for column mapping in rendering
        // For deterministic_extract, fields come from output_schema.fields (object type) or output_schema.items.fields (array type)
        const outputSchema = firstOp.output_schema
        const aiOutputFields: string[] = []
        if (outputSchema) {
          const fieldsArray = outputSchema.type === 'array'
            ? outputSchema.items?.fields
            : outputSchema.fields
          if (fieldsArray && Array.isArray(fieldsArray)) {
            for (const field of fieldsArray) {
              if (field.name && !aiOutputFields.includes(field.name)) {
                aiOutputFields.push(field.name)
              }
            }
          }
        }
        if (aiOutputFields.length > 0) {
          ctx.aiOutputFields = aiOutputFields
          this.log(ctx, `✓ Captured ${aiOutputFields.length} extraction output fields for column mapping: ${aiOutputFields.join(', ')}`)
        }

        // ✅ Add flatten transform to extract actual data from AI results
        // The scatter-gather returns an array of extraction results, where each result has:
        // { filename: "...", data: [...extracted items...], confidence: ..., metadata: ... }
        //
        // We ALWAYS need to extract the 'data' field and flatten it, regardless of output_schema.type
        // - For 'object' type: each result.data is a single object → flatten gives array of objects
        // - For 'array' type: each result.data is an array → flatten gives flat array of all items
        const fieldToExtract = 'data'
        this.log(ctx, `✓ AI output schema type: ${outputSchema.type} - will extract and flatten 'data' field from extraction results`)

        const flattenMetadata = this.generateStepMetadata('flatten_results', 'Flatten AI Results', ctx)
        steps.push({
          ...flattenMetadata,
          type: 'transform',
          operation: 'flatten',
          input: `{{${ctx.currentVariable}}}`,
          config: {
            field: fieldToExtract
          }
        })

        // Transform step outputs are stored in .data
        ctx.currentVariable = `${flattenMetadata.id}.data`
        this.log(ctx, `✓ Added flatten transform to extract field '${fieldToExtract}' from AI extraction results`)
      }

      // Save the extracted rows variable for later rendering
      // (extractedDataVariable was initialized at the start of this block)
      extractedDataVariable = ctx.currentVariable

      // STEP 2: Process summarization AI operations (work on flattened data)
      const summarizeOps = ir.ai_operations.filter(op =>
        op.type === 'summarize'
      )

      if (summarizeOps.length > 0) {
        this.log(ctx, `Processing ${summarizeOps.length} summarization operations`)

        summarizeOps.forEach((summarizeOp) => {
          const summarizeStep = this.compileAIOperation(summarizeOp, extractedDataVariable, ctx)

          // Summarization doesn't need a loop - it works on the entire dataset
          steps.push({
            step_id: summarizeStep.step_id,
            type: summarizeStep.type,
            config: {
              ...summarizeStep.config,
              input: `{{${extractedDataVariable}}}` // Pass flattened extraction results
            },
            output_variable: summarizeStep.output_variable
          })

          this.log(ctx, `✓ Added summarization AI operation: ${summarizeOp.type}`)
        })

        // Don't update ctx.currentVariable - keep it pointing to extracted data for rendering
      }

      // STEP 3: Process transform/classify AI operations (semantic processing of data)
      // Decision: per-item (scatter-gather) vs batch (single call) based on context/instruction
      const semanticOps = ir.ai_operations.filter(op =>
        op.type === 'transform' || op.type === 'classify' || op.type === 'generate' ||
        op.type === 'sentiment' || op.type === 'enrich' || op.type === 'normalize' ||
        op.type === 'validate' || op.type === 'decide'
      )

      if (semanticOps.length > 0) {
        this.log(ctx, `Processing ${semanticOps.length} semantic AI operations (transform/classify/etc.)`)

        // Check if primary data source returns a single result object (not an array)
        // Single-result plugins (research, AI) should ALWAYS use batch processing
        // because there's nothing to iterate over - the whole result is one object
        const primaryPlugin = ctx.primaryDataSourcePlugin || ''
        const isSingleResultSource = this.pluginReturnsSingleResultObject(primaryPlugin)

        if (isSingleResultSource) {
          this.log(ctx, `✓ Primary data source (${primaryPlugin}) returns single result - forcing batch processing (no scatter-gather)`)
        }

        // Helper: Detect if operation should be per-item (scatter-gather) or batch
        // Per-item indicators: "each", "per", "every", "individual" in instruction
        // Batch indicators (default): "all", "sources", "together", or no specific indicator
        const needsPerItemProcessing = (op: any): boolean => {
          // Single-result sources NEVER need per-item processing
          // (there's only one item - the full result object)
          if (isSingleResultSource) {
            return false
          }

          const instruction = (op.instruction || '').toLowerCase()
          const context = this.getContextString(op.context).toLowerCase()

          // Per-item keywords - process each item separately
          const perItemKeywords = ['each email', 'each item', 'each record', 'each row',
            'per email', 'per item', 'per record', 'every email', 'every item',
            'individual email', 'individually']

          for (const keyword of perItemKeywords) {
            if (instruction.includes(keyword) || context.includes(keyword)) {
              return true
            }
          }

          // Default: batch processing (single AI call with all data)
          // This is safer and works for most cases like research sources
          return false
        }

        // Separate ops into per-item and batch
        const perItemOps = semanticOps.filter(needsPerItemProcessing)
        const batchOps = semanticOps.filter(op => !needsPerItemProcessing(op))

        // Process per-item operations with scatter-gather
        if (perItemOps.length > 0) {
          this.log(ctx, `Processing ${perItemOps.length} per-item semantic AI operations (scatter-gather)`)

          const scatterSteps: any[] = []
          perItemOps.forEach((semanticOp) => {
            const aiStep = this.compileAIOperation(semanticOp, 'item', ctx)
            scatterSteps.push(aiStep)
            this.log(ctx, `✓ Added per-item AI operation to scatter: ${semanticOp.type}`)
          })

          const loopSemanticMetadata = this.generateStepMetadata('loop_semantic_ai', 'Loop Semantic AI Processing', ctx)
          steps.push({
            ...loopSemanticMetadata,
            type: 'scatter_gather',
            scatter: {
              input: `{{${extractedDataVariable}}}`,
              itemVariable: 'item',
              steps: scatterSteps
            },
            gather: { operation: 'collect' },
            output_variable: 'ai_semantic_results'
          })

          ctx.currentVariable = 'ai_semantic_results'
          this.log(ctx, `✓ Created scatter-gather loop for per-item semantic AI processing`)
        }

        // Process batch operations - single AI call with all data (default behavior)
        if (batchOps.length > 0) {
          this.log(ctx, `Processing ${batchOps.length} batch semantic AI operations (single call)`)

          batchOps.forEach((semanticOp) => {
            const aiStep = this.compileAIOperation(semanticOp, extractedDataVariable, ctx)

            // Add the AI step directly (no scatter-gather needed)
            steps.push({
              ...aiStep,
              // Ensure input references the full data array
              config: {
                ...aiStep.config,
                input: `{{${extractedDataVariable}}}`
              }
            })

            // Update current variable to AI output for next step
            // Use stepId.data pattern which is what the execution engine expects
            ctx.currentVariable = `${aiStep.id}.data`
            this.log(ctx, `✓ Added batch AI operation: ${semanticOp.type} (output: ${ctx.currentVariable})`)
          })
        }
      }
    }

    // Step: Apply post-AI filters (filter on AI output fields)
    // Example: Only show emails where action_required = true
    if (ir.post_ai_filters && (ir.post_ai_filters.conditions || ir.post_ai_filters.groups)) {
      const postAIFilterSteps = this.compilePostAIFilters(ir, ctx)
      steps.push(...postAIFilterSteps)
    }

    // Step: Apply sorting (order output data before rendering)
    // Example: Sort by Priority desc, then by Received time desc
    if (ir.rendering?.sort_order && ir.rendering.sort_order.length > 0) {
      const sortSteps = this.compileSorting(ir, ctx)
      steps.push(...sortSteps)
    }

    // Calculate summary statistics if requested
    let summaryVariable: string | null = null
    if (ir.rendering?.summary_stats && ir.rendering.summary_stats.length > 0) {
      const summaryStats = this.compileSummaryStats(
        ir.rendering.summary_stats,
        ctx.currentVariable,
        ctx
      )
      if (summaryStats.length > 0) {
        steps.push(...summaryStats)
        // Transform step outputs are stored in .data
        const lastStepId = summaryStats[summaryStats.length - 1].id || summaryStats[summaryStats.length - 1].step_id
        summaryVariable = `${lastStepId}.data`
        this.log(ctx, `✓ Added ${ir.rendering.summary_stats.length} summary statistics calculations`)
      }
    }

    // Render if needed (render the EXTRACTED DATA, not the summary!)
    if (ir.rendering) {
      // Map IR column names to actual plugin output field names
      let mappedColumns = ir.rendering.columns_in_order
      if (ctx.primaryDataSourcePlugin && ctx.primaryDataSourceOperation && mappedColumns) {
        mappedColumns = this.mapColumnsToPluginFields(
          mappedColumns,
          ctx.primaryDataSourcePlugin,
          ctx.primaryDataSourceOperation,
          ctx
        )
      }

      // ✅ FIX: When AI extraction is the primary data source, USE AI output fields ONLY
      // The IR columns_in_order represent user-specified names, but after extraction,
      // the actual data fields are from the extraction output_schema
      if (ctx.aiOutputFields && ctx.aiOutputFields.length > 0) {
        const hasDeterministicExtraction = ir.ai_operations?.some(op => op.type === 'deterministic_extract')

        if (hasDeterministicExtraction) {
          // Replace columns entirely with AI output fields - the IR columns are stale
          this.log(ctx, `✓ Deterministic extraction detected - using AI output fields as columns: ${ctx.aiOutputFields.join(', ')}`)
          mappedColumns = [...ctx.aiOutputFields]
        } else {
          // For other AI operations (classify, transform), append AI fields to existing columns
          const existingColumns = new Set((mappedColumns || []).map(c => c.toLowerCase()))
          const newAIFields = ctx.aiOutputFields.filter(f => !existingColumns.has(f.toLowerCase()))

          if (newAIFields.length > 0) {
            mappedColumns = [...(mappedColumns || []), ...newAIFields]
            this.log(ctx, `✓ Added ${newAIFields.length} AI output fields to columns: ${newAIFields.join(', ')}`)
          }
        }
      }

      // Check if delivery target is Google Sheets - needs 2D array, not rendered string
      const deliveryPlugin = summary_delivery?.plugin_key?.toLowerCase() || ''
      const isSheetsDelivery = deliveryPlugin.includes('sheets') || deliveryPlugin.includes('spreadsheet')

      if (isSheetsDelivery && mappedColumns && mappedColumns.length > 0) {
        // For Sheets: Use map transform with columns to produce 2D array
        const prepareMetadata = this.generateStepMetadata('prepare_sheets_data', 'Prepare Sheets Data', ctx)

        // Build config with conditional headers
        const prepareConfig: any = {
          columns: mappedColumns,
          add_headers: true
        }

        // ✅ Only add headers if reference sheet is empty (prevents duplicate headers on each run)
        if (ctx.referenceDataVariable) {
          prepareConfig.add_headers_source = `{{${ctx.referenceDataVariable}}}`
          this.log(ctx, `✓ Will only add headers if ${ctx.referenceDataVariable} is empty`)
        }

        steps.push({
          ...prepareMetadata,
          type: 'transform',
          operation: 'map',
          input: `{{${ctx.currentVariable}}}`,
          config: prepareConfig
        })

        ctx.currentVariable = `${prepareMetadata.id}.data`
        this.log(ctx, `✓ Prepared data for Sheets delivery (2D array with ${mappedColumns.length} columns)`)
      } else {
        // Check if AI transform already produced HTML output (skip redundant render)
        // This happens when the AI instruction explicitly asks for HTML formatting
        const aiProducedHtml = this.didAIProduceHtmlOutput(ir)

        if (aiProducedHtml) {
          // AI already produced HTML - skip render_table, use AI output directly
          // AI transform output is in the 'result' field of the step output
          // Update currentVariable to point directly to that field
          ctx.currentVariable = `${ctx.currentVariable}.result`
          this.log(ctx, `✓ AI already produced HTML output - using ${ctx.currentVariable} directly (skipping render_table)`)
        } else {
          // For other targets (email, slack, etc.): Use render_table for HTML/text output
          const renderMetadata = this.generateStepMetadata('render_table', 'Render Table', ctx)

          const renderConfig: any = {
            rendering_type: ir.rendering.type,
            columns: mappedColumns,
            empty_message: ir.rendering.empty_message
          }

          // Include summary stats in rendering if available
          if (summaryVariable) {
            renderConfig.summary_variable = summaryVariable
          }

          steps.push({
            ...renderMetadata,
            type: 'transform',
            operation: 'render_table',
            input: `{{${ctx.currentVariable}}}`,
            config: renderConfig
          })

          ctx.currentVariable = `${renderMetadata.id}.data`
          this.log(ctx, '✓ Added table rendering from extracted data')
        }
      }
    }

    // Send summary via plugin (email, slack, etc.)
    if (!summary_delivery!.plugin_key) {
      throw new Error('summary_delivery.plugin_key is required for compilation')
    }

    const deliveryPluginKey = summary_delivery!.plugin_key
    const deliveryOpType = summary_delivery!.operation_type || 'send'

    // Resolve delivery plugin with error handling
    let deliveryResolution
    try {
      deliveryResolution = this.pluginResolver.resolveDelivery(deliveryPluginKey, deliveryOpType)
    } catch (error) {
      const errorMsg = `Failed to resolve summary delivery plugin: ${deliveryPluginKey}.${deliveryOpType}`
      this.log(ctx, `✗ ${errorMsg}`)
      throw new Error(`${errorMsg}: ${error instanceof Error ? error.message : String(error)}`)
    }

    // Build delivery params using PluginResolver (plugin-agnostic!)
    // This ensures correct parameters for ANY plugin (email, sheets, slack, etc.)
    const deliveryParams = this.buildDeliveryParams(
      summary_delivery!,
      deliveryResolution.parameters_schema,
      ctx,
      ctx.currentVariable
    )

    const summaryMetadata = this.generateStepMetadata('send_summary', `Send Summary via ${deliveryPluginKey}`, ctx)
    steps.push({
      ...summaryMetadata,
      type: 'action',
      plugin: deliveryResolution.plugin_name,
      action: deliveryResolution.operation,  // Use 'action' for PILOT executor compatibility
      params: deliveryParams
    })

    this.log(ctx, `✓ Added summary delivery via ${deliveryResolution.plugin_name}.${deliveryResolution.operation}`)

    return steps
  }

  // ==========================================================================
  // Multi-Destination Delivery Pattern
  // ==========================================================================

  /**
   * Compile multi-destination delivery pattern
   * Sends the same rendered data to multiple destinations in parallel
   * Example: Email + Slack + Google Sheets simultaneously
   */
  private compileMultiDestinationDelivery(ir: DeclarativeLogicalIR, ctx: CompilerContext): WorkflowStep[] {
    const steps: WorkflowStep[] = []
    const { multiple_destinations } = ir.delivery_rules

    if (!multiple_destinations || multiple_destinations.length === 0) {
      throw new Error('multiple_destinations is required but empty')
    }

    this.log(ctx, `Compiling multi-destination delivery to ${multiple_destinations.length} destinations`)

    // Step 1: Process AI operations using shared helper
    // This handles: file-based extraction, summarization, and semantic ops (classify, transform, extract, generate, etc.)
    const aiResult = this.processAIOperationsForDelivery(ir, ctx)
    steps.push(...aiResult.steps)
    ctx.currentVariable = aiResult.currentVariable

    // Step 1.5: Apply post-AI filters (filter on AI output fields)
    // Example: Only show emails where action_required = true
    if (ir.post_ai_filters && (ir.post_ai_filters.conditions || ir.post_ai_filters.groups)) {
      const postAIFilterSteps = this.compilePostAIFilters(ir, ctx)
      steps.push(...postAIFilterSteps)
    }

    // Step 1.6: Apply sorting (order output data before rendering)
    // Example: Sort by Priority desc, then by Received time desc
    if (ir.rendering?.sort_order && ir.rendering.sort_order.length > 0) {
      const sortSteps = this.compileSorting(ir, ctx)
      steps.push(...sortSteps)
    }

    // Step 2: Render data if needed
    if (ir.rendering) {
      // ✅ CRITICAL: Keep TWO sets of columns:
      // 1. semanticColumns - original names from IR for display headers (e.g., "Sender", "Received time")
      // 2. dataColumns - mapped plugin field names for data extraction (e.g., "from", "date")
      const semanticColumns = ir.rendering.columns_in_order || []
      let dataColumns = [...semanticColumns]

      // Map semantic columns to plugin field names for data extraction
      if (ctx.primaryDataSourcePlugin && ctx.primaryDataSourceOperation && dataColumns.length > 0) {
        dataColumns = this.mapColumnsToPluginFields(
          dataColumns,
          ctx.primaryDataSourcePlugin,
          ctx.primaryDataSourceOperation,
          ctx
        )
      }

      // ✅ FIX: Deduplicate data columns (IR may contain duplicates from Phase 3)
      if (dataColumns.length > 0) {
        const seen = new Set<string>()
        const deduped: string[] = []
        for (const col of dataColumns) {
          const key = col.toLowerCase()
          if (!seen.has(key)) {
            seen.add(key)
            deduped.push(col)
          }
        }
        if (deduped.length < dataColumns.length) {
          this.log(ctx, `✓ Removed ${dataColumns.length - deduped.length} duplicate column(s)`)
          dataColumns = deduped
        }
      }

      // Build column mapping: semantic name → data field name
      // This allows render_table to use semantic names for headers while extracting from data fields
      const columnMapping: Record<string, string> = {}
      for (let i = 0; i < semanticColumns.length && i < dataColumns.length; i++) {
        if (semanticColumns[i].toLowerCase() !== dataColumns[i].toLowerCase()) {
          columnMapping[semanticColumns[i]] = dataColumns[i]
        }
      }

      if (Object.keys(columnMapping).length > 0) {
        this.log(ctx, `✓ Column mapping (semantic → data): ${JSON.stringify(columnMapping)}`)
      }

      // Detect destination data format needs from plugin parameter schemas (plugin-agnostic)
      // - Tabular: plugin accepts 'values', 'rows', or 'records' (array of arrays)
      // - Content: plugin accepts 'body', 'html_body', 'message', 'text', 'content' (rendered string)
      let hasTabularDestination = false
      let hasContentDestination = false

      for (const dest of multiple_destinations) {
        try {
          const resolution = this.pluginResolver.resolveDelivery(dest.plugin_key, dest.operation_type as any)
          const params = resolution.parameters_schema?.properties || {}
          const paramNames = Object.keys(params).map(p => p.toLowerCase())

          // Check parameter names to determine data format needs
          const needsTabular = paramNames.some(p => ['values', 'rows', 'records', 'data'].includes(p))
          const needsContent = paramNames.some(p => ['body', 'html_body', 'message', 'text', 'content'].includes(p))

          if (needsTabular) hasTabularDestination = true
          if (needsContent) hasContentDestination = true
        } catch {
          // Default to content if plugin resolution fails
          hasContentDestination = true
        }
      }

      const sourceVariable = ctx.currentVariable
      let tabularDataVariable: string | null = null
      let contentDataVariable: string | null = null

      // Create tabular transform if needed (2D array for spreadsheet-like plugins)
      // Use semantic columns for headers, data columns for extraction
      if (hasTabularDestination && dataColumns.length > 0) {
        const prepareMetadata = this.generateStepMetadata('prepare_tabular_data', 'Prepare Tabular Data', ctx)

        const prepareConfig: any = {
          columns: dataColumns,              // Data field names for extraction
          header_names: semanticColumns,     // Semantic names for headers
          add_headers: true
        }

        if (ctx.referenceDataVariable) {
          prepareConfig.add_headers_source = `{{${ctx.referenceDataVariable}}}`
          this.log(ctx, `✓ Will only add headers if ${ctx.referenceDataVariable} is empty`)
        }

        steps.push({
          ...prepareMetadata,
          type: 'transform',
          operation: 'map',
          input: `{{${sourceVariable}}}`,
          config: prepareConfig
        })

        tabularDataVariable = `${prepareMetadata.id}.data`
        this.log(ctx, `✓ Prepared tabular data (2D array with ${dataColumns.length} columns, semantic headers: ${semanticColumns.slice(0, 3).join(', ')}...)`)
      }

      // Create content transform if needed (HTML table for messaging-like plugins)
      // Use semantic columns for display headers, data columns for extraction
      if (hasContentDestination && dataColumns.length > 0) {
        const renderMetadata = this.generateStepMetadata('render_content', 'Render HTML Table', ctx)

        steps.push({
          ...renderMetadata,
          type: 'transform',
          operation: 'render_table',
          input: `{{${sourceVariable}}}`,
          config: {
            rendering_type: 'html_table',
            columns: dataColumns,            // Data field names for extraction
            header_names: semanticColumns,   // Semantic names for display
            column_mapping: Object.keys(columnMapping).length > 0 ? columnMapping : undefined,
            empty_message: ir.rendering?.empty_message || 'No data to display'
          }
        })

        contentDataVariable = `${renderMetadata.id}.data`
        this.log(ctx, `✓ Prepared content data (HTML table with semantic headers: ${semanticColumns.slice(0, 3).join(', ')}...)`)
      }

      // Store variables on context for use in delivery params
      ctx.tabularDataVariable = tabularDataVariable
      ctx.contentDataVariable = contentDataVariable
    }

    // Step 3: Variables are stored on ctx (tabularDataVariable, contentDataVariable)
    // Step 4: Create parallel delivery actions for each destination
    const parallelActions: WorkflowStep[] = []

    for (let i = 0; i < multiple_destinations.length; i++) {
      const destination = multiple_destinations[i]
      const destinationName = destination.name || `Destination ${i + 1}`

      this.log(ctx, `Processing destination ${i + 1}/${multiple_destinations.length}: ${destinationName} (${destination.plugin_key})`)

      // Validate required fields
      if (!destination.plugin_key) {
        throw new Error(`plugin_key is required for destination: ${destinationName}`)
      }
      if (!destination.operation_type) {
        throw new Error(`operation_type is required for destination: ${destinationName}`)
      }

      const deliveryPluginKey = destination.plugin_key
      const deliveryOpType = destination.operation_type as 'send' | 'post' | 'publish'

      // Resolve delivery plugin with error handling
      let deliveryResolution
      try {
        deliveryResolution = this.pluginResolver.resolveDelivery(deliveryPluginKey, deliveryOpType)
      } catch (error) {
        const errorMsg = `Failed to resolve multi-destination delivery plugin: ${deliveryPluginKey}.${deliveryOpType} for ${destinationName}`
        this.log(ctx, `✗ ${errorMsg}`)
        throw new Error(`${errorMsg}: ${error instanceof Error ? error.message : String(error)}`)
      }

      // Determine which data variable to use based on plugin's parameter schema
      const params = deliveryResolution.parameters_schema?.properties || {}
      const paramNames = Object.keys(params).map(p => p.toLowerCase())
      const needsTabular = paramNames.some(p => ['values', 'rows', 'records', 'data'].includes(p))

      // Use tabular or content data variable based on what this destination accepts
      const dataVariableForDestination = needsTabular
        ? (ctx.tabularDataVariable || ctx.currentVariable)
        : (ctx.contentDataVariable || ctx.currentVariable)

      // Build delivery params using plugin-agnostic mapping
      const deliveryParams = this.buildDeliveryParams(
        destination,
        deliveryResolution.parameters_schema,
        ctx,
        dataVariableForDestination
      )

      // Create delivery action step
      const deliveryMetadata = this.generateStepMetadata(
        `send_to_${deliveryPluginKey}_${i + 1}`,
        `Send to ${destinationName}`,
        ctx
      )

      parallelActions.push({
        ...deliveryMetadata,
        type: 'action',
        plugin: deliveryResolution.plugin_name,
        action: deliveryResolution.operation,  // Use 'action' for PILOT executor compatibility
        params: deliveryParams
      })

      this.log(ctx, `✓ Added delivery action for ${destinationName}: ${deliveryResolution.plugin_name}.${deliveryResolution.operation}`)
    }

    // Step 5: Execute all deliveries in parallel
    // ✅ FIX: Instead of scatter_gather with all actions inside (which would run ALL actions for EACH index),
    // use a 'parallel' step that runs all actions concurrently without iteration
    if (parallelActions.length === 1) {
      // Single destination - just add the action directly
      steps.push(parallelActions[0])
      this.log(ctx, `✓ Added single delivery action`)
    } else {
      // Multiple destinations - use parallel step to run all concurrently
      const parallelDeliveryMetadata = this.generateStepMetadata('parallel_delivery', 'Execute Parallel Deliveries', ctx)

      steps.push({
        ...parallelDeliveryMetadata,
        type: 'parallel',
        steps: parallelActions,
        output_variable: 'multi_delivery_results'
      })

      this.log(ctx, `✓ Created parallel execution of ${parallelActions.length} delivery actions`)
    }

    return steps
  }

  // ==========================================================================
  // Write Operations Compilation
  // ==========================================================================

  /**
   * Compile WRITE operations from data_sources
   * Detects write operations by checking operation_type, not role (more robust)
   */
  private compileWriteOperations(ir: DeclarativeLogicalIR, ctx: CompilerContext): WorkflowStep[] {
    const steps: WorkflowStep[] = []

    // Filter for write operations based on operation_type (not role, which varies)
    // This is more robust since gpt-5.2 can use any semantic role name
    const writeOperationTypes = ['write', 'append', 'update', 'upsert', 'delete']
    const writeTargets = ir.data_sources.filter(ds =>
      ds.operation_type && writeOperationTypes.includes(ds.operation_type)
    )

    if (writeTargets.length === 0) {
      this.log(ctx, 'No write operations found')
      return steps
    }

    this.log(ctx, `Found ${writeTargets.length} write operation(s)`)

    for (const writeTarget of writeTargets) {
      // Determine plugin key and operation type
      if (!writeTarget.plugin_key) {
        throw new Error(`data_source.plugin_key is required for write operation: ${writeTarget.source}`)
      }

      const pluginKey = writeTarget.plugin_key
      const operationType = writeTarget.operation_type || 'write'

      // Resolve plugin action using PluginResolver with error handling
      let resolution
      try {
        resolution = this.pluginResolver.resolveDataSource(pluginKey, operationType)
      } catch (error) {
        const errorMsg = `Failed to resolve write operation plugin: ${pluginKey}.${operationType}`
        this.log(ctx, `✗ ${errorMsg}`)
        throw new Error(`${errorMsg}: ${error instanceof Error ? error.message : String(error)}`)
      }

      this.log(ctx, `✓ Resolved write operation: ${pluginKey}.${operationType} → ${resolution.plugin_name}.${resolution.operation}`)

      // Build params generically from schema and data source (plugin-agnostic!)
      const params = this.buildDataSourceParams(writeTarget, resolution.parameters_schema, ctx, ctx.currentVariable)

      // Generate write action step
      const metadata = this.generateStepMetadata(`write_${writeTarget.source}`, `Write to ${writeTarget.source}`, ctx)
      steps.push({
        ...metadata,
        type: 'action',
        plugin: resolution.plugin_name,
        action: resolution.operation,  // Use 'action' for PILOT executor compatibility
        params
      })

      this.log(ctx, `✓ Compiled write operation: ${resolution.plugin_name}.${resolution.operation} to ${writeTarget.location}`)
    }

    return steps
  }

  // ==========================================================================
  // AI Operation Compilation
  // ==========================================================================

  /**
   * Compile AI operation to ai_processing step
   * Maps IR AI operations (extract, summarize, classify, etc.) to PILOT ai_processing steps
   *
   * NOTE: Uses ai_processing type (not action with chatgpt-research) to enable:
   * - Vision/multimodal support via ExtractHandler
   * - Orchestration with proper token routing
   * - Structured JSON output parsing
   */
  private compileAIOperation(aiOp: AIOperation, itemVar: string, ctx: CompilerContext): any {
    const stepId = this.generateStepId(`ai_${aiOp.type}`, ctx)

    // Handle deterministic_extract separately - uses OCR/text extraction followed by LLM for structured fields
    // This approach: OCR (free) → LLM extracts all fields from raw text (more accurate than pattern matching)
    // Supports flexible output types based on user intent: object (default), array (line items), string (summary)
    if (aiOp.type === 'deterministic_extract') {
      const outputType = aiOp.output_schema?.type || 'object'
      this.log(ctx, `✓ Using deterministic extraction (OCR + LLM) for document extraction (output: ${outputType})`)

      // Build output_schema preserving type for flexible output formats
      const outputSchema = aiOp.output_schema ? {
        type: outputType,
        // For object type: fields at top level
        fields: outputType !== 'array' ? (aiOp.output_schema.fields || []).map((f: any) => ({
          name: f.name,
          type: f.type || 'string',
          required: f.required || false,
          description: f.description || ''
        })) : undefined,
        // For array type: fields inside items
        items: outputType === 'array' ? {
          fields: (aiOp.output_schema.fields || aiOp.output_schema.items?.fields || []).map((f: any) => ({
            name: f.name,
            type: f.type || 'string',
            required: f.required || false,
            description: f.description || ''
          }))
        } : undefined,
        // For string type or overall context
        description: aiOp.output_schema.description
      } : undefined

      return {
        id: stepId,
        name: `Deterministic ${aiOp.document_type || 'document'} extraction`,
        type: 'deterministic_extraction',
        input: `{{${itemVar}}}`,
        document_type: aiOp.document_type || 'auto',
        ocr_fallback: aiOp.ocr_fallback !== false, // Default to true
        output_schema: outputSchema,
        instruction: aiOp.instruction, // Pass through the extraction instruction
        output_variable: `extracted_${aiOp.document_type || 'data'}`
      }
    }

    // Build the prompt for ai_processing
    let prompt = aiOp.instruction

    // If instruction doesn't reference the input variable, append it
    if (!prompt.includes('{{')) {
      prompt = `${prompt}\n\nInput data:\n{{${itemVar}}}`
    } else {
      // Replace any {{item}} or {{input}} placeholders with the actual variable
      prompt = prompt.replace(/\{\{item\}\}/g, `{{${itemVar}}}`)
      prompt = prompt.replace(/\{\{input\}\}/g, `{{${itemVar}}}`)
    }

    // For extraction operations, format the output schema as structured extraction instruction
    if (aiOp.type === 'extract' && aiOp.output_schema) {
      const schemaFields = aiOp.output_schema.fields || []
      const fieldDescriptions = schemaFields.map((f: any) =>
        `- ${f.name} (${f.type}${f.required ? ', required' : ''}): ${f.description || ''}`
      ).join('\n')

      prompt = `${aiOp.instruction}

Output MUST be a JSON object with these exact fields:
${fieldDescriptions}

Input data:
{{${itemVar}}}`
    }

    // Use ai_processing type for proper orchestration routing
    // This enables vision support via ExtractHandler and structured output
    return {
      id: stepId,
      name: `AI ${aiOp.type} operation`,
      type: 'ai_processing',
      prompt: prompt,
      // Include context from IR if available (helps with vision detection)
      description: typeof aiOp.context === 'string' ? aiOp.context : aiOp.instruction,
      // Pass output schema for structured extraction
      output_schema: aiOp.output_schema,
      output_variable: `ai_${aiOp.type}_result`
    }
  }

  // ==========================================================================
  // Helper Functions
  // ==========================================================================

  /**
   * Build params dynamically from DataSource IR and plugin parameter schema
   * FULLY PLUGIN-AGNOSTIC - uses schema to determine what parameters are needed
   */
  private buildDataSourceParams(
    dataSource: DataSource,
    parametersSchema: any,
    ctx: CompilerContext,
    currentVariable?: string
  ): Record<string, any> {
    const params: Record<string, any> = {}

    if (!parametersSchema || !parametersSchema.properties) {
      return params
    }

    const properties = parametersSchema.properties

    // Generic parameter mapping from IR to plugin parameters
    for (const [paramName, paramSchema] of Object.entries(properties) as [string, any][]) {
      const paramNameLower = paramName.toLowerCase()

      // PRIORITY 1: Use IR config if provided (populated by IRFormalizer)
      // This is the highest priority - allows Phase 3 to directly populate plugin parameters
      if (dataSource.config && paramName in dataSource.config && dataSource.config[paramName] !== null) {
        params[paramName] = dataSource.config[paramName]
        continue
      }

      // Data/content parameters - map to current pipeline variable
      if (currentVariable && (
        paramNameLower.includes('data') ||
        paramNameLower.includes('values') ||
        paramNameLower.includes('rows') ||
        paramNameLower.includes('content') ||
        paramNameLower.includes('body')
      )) {
        params[paramName] = `{{${currentVariable}}}`
        continue
      }

      // Location/identifier parameters - map from IR or reuse from stored configs
      if (paramNameLower.includes('id') || paramNameLower.includes('identifier')) {
        // PRIORITY 2: Check if we have stored config from a previous read operation (e.g., deduplication reference source)
        // This handles cases where read and write operations use the same data source
        if (ctx.dataSourceConfigs.has(dataSource.source)) {
          const storedConfig = ctx.dataSourceConfigs.get(dataSource.source)
          if (storedConfig[paramName]) {
            params[paramName] = storedConfig[paramName]
            this.log(ctx, `✓ Reusing ${paramName} from stored config: ${storedConfig[paramName]}`)
            continue
          }
        }

        // PRIORITY 3: Default to workflow inputs
        params[paramName] = `{{inputs.${paramName}}}`
        continue
      }

      // Range/tab/sheet parameters
      if (dataSource.tab && (paramNameLower.includes('range') || paramNameLower.includes('sheet') || paramNameLower.includes('tab'))) {
        if (paramNameLower.includes('range')) {
          params[paramName] = `${dataSource.tab}!A:Z`
        } else {
          params[paramName] = dataSource.tab
        }
        continue
      }

      // Endpoint/path parameters
      if (dataSource.endpoint && (paramNameLower.includes('endpoint') || paramNameLower.includes('path') || paramNameLower.includes('url'))) {
        params[paramName] = dataSource.endpoint
        continue
      }

      // Location parameter (generic)
      if (paramNameLower.includes('location') || paramNameLower.includes('table') || paramNameLower.includes('collection')) {
        params[paramName] = dataSource.location
        continue
      }

      // Use schema default if available
      if (paramSchema.default !== undefined) {
        params[paramName] = paramSchema.default
        continue
      }

      // Common optional parameters with reasonable defaults
      // Add warning logs so we know when defaults are used
      if (paramNameLower.includes('limit') || paramNameLower.includes('max')) {
        this.log(ctx, `⚠ Using default value 100 for parameter '${paramName}' (not found in IR config)`)
        params[paramName] = 100
      } else if (paramNameLower.includes('include') && paramSchema.type === 'boolean') {
        this.log(ctx, `⚠ Using default value true for parameter '${paramName}' (not found in IR config)`)
        params[paramName] = true
      }
    }

    return params
  }

  /**
   * Build delivery parameters from delivery rule and parameter schema
   * Plugin-agnostic parameter mapping for ANY delivery method (email, sheets, slack, etc.)
   * Handles both flat and nested parameter schemas (e.g., google-mail's recipients.to, content.subject)
   */
  private buildDeliveryParams(
    deliveryRule: any,
    parametersSchema: any,
    ctx: CompilerContext,
    currentVariable: string
  ): Record<string, any> {
    const params: Record<string, any> = {}

    if (!parametersSchema || !parametersSchema.properties) {
      return params
    }

    const properties = parametersSchema.properties

    // Derive the input base name from plugin_key (for consistent naming with DSLWrapper)
    // e.g., "google-sheets" → "google_sheets", "google-mail" → "google_mail"
    const pluginKey = deliveryRule.plugin_key || ''
    const inputBaseName = pluginKey.replace(/-/g, '_')

    // Check if this plugin uses nested object structure (like google-mail send_email)
    const hasRecipientsObject = 'recipients' in properties && properties.recipients.type === 'object'
    const hasContentObject = 'content' in properties && properties.content.type === 'object'

    // Handle nested email structure (google-mail send_email pattern)
    if (hasRecipientsObject && hasContentObject) {
      // Build recipients object
      const recipients: any = {}
      if (properties.recipients.properties) {
        if ('to' in properties.recipients.properties) {
          recipients.to = deliveryRule.recipient ? [deliveryRule.recipient] : []
        }
        if ('cc' in properties.recipients.properties && deliveryRule.cc) {
          recipients.cc = deliveryRule.cc
        }
        if ('bcc' in properties.recipients.properties && deliveryRule.bcc) {
          recipients.bcc = deliveryRule.bcc
        }
      }
      params.recipients = recipients

      // Build content object
      const content: any = {}
      if (properties.content.properties) {
        if ('subject' in properties.content.properties) {
          content.subject = deliveryRule.subject || 'Workflow Results'
        }
        if ('body' in properties.content.properties) {
          content.body = `{{${currentVariable}}}`
        }
        if ('html_body' in properties.content.properties) {
          content.html_body = `{{${currentVariable}}}`
        }
      }
      params.content = content

      // Handle options object if present
      if ('options' in properties && properties.options.type === 'object') {
        const options: any = {}
        if (properties.options.properties) {
          // Map any options with defaults
          for (const [optName, optSchema] of Object.entries(properties.options.properties) as [string, any][]) {
            if (optSchema.default !== undefined) {
              options[optName] = optSchema.default
            }
          }
        }
        if (Object.keys(options).length > 0) {
          params.options = options
        }
      }

      return params
    }

    // Handle flat parameter structure (for other plugins like google-sheets, slack, etc.)
    for (const [paramName, paramSchema] of Object.entries(properties) as [string, any][]) {
      const paramNameLower = paramName.toLowerCase()

      // PRIORITY 1: Check if parameter is in IR config (Phase 3 can populate this)
      // Skip null values AND empty arrays - empty arrays should fall through to default handling
      // which sets proper variable references like "{{step9.data}}"
      // ✅ CRITICAL: Skip 'values', 'rows', 'data' params from IR config - these should ALWAYS
      // reference workflow data (e.g., {{step4.data}}), never static values from IR
      const dynamicDataParams = ['values', 'rows', 'data', 'body', 'html_body', 'message', 'text']
      if (deliveryRule.config && paramName in deliveryRule.config && !dynamicDataParams.includes(paramNameLower)) {
        const configValue = deliveryRule.config[paramName]
        const isEmptyArray = Array.isArray(configValue) && configValue.length === 0
        if (configValue !== null && !isEmptyArray) {
          params[paramName] = configValue
          continue
        }
      }

      // PRIORITY 2: Reuse from stored data source configs (plugin-agnostic)
      // This handles write-to-same-source pattern: if we read from a source, reuse its config for writes
      // Iterate through all stored configs to find a match for this parameter
      for (const [sourceName, storedConfig] of ctx.dataSourceConfigs.entries()) {
        if (storedConfig && paramName in storedConfig && storedConfig[paramName] !== null) {
          params[paramName] = storedConfig[paramName]
          this.log(ctx, `✓ Reusing ${paramName} from stored config (${sourceName}): ${storedConfig[paramName]}`)
          break
        }
      }
      if (params[paramName] !== undefined) {
        continue
      }

      // Email-specific flat parameters (for plugins that don't use nested structure)
      if (paramNameLower === 'to' || paramNameLower === 'recipients') {
        params[paramName] = deliveryRule.recipient ? [deliveryRule.recipient] : []
        continue
      }

      if (paramNameLower === 'cc') {
        params[paramName] = deliveryRule.cc || []
        continue
      }

      if (paramNameLower === 'subject') {
        params[paramName] = deliveryRule.subject || 'Workflow Results'
        continue
      }

      if (paramNameLower.includes('body') || paramNameLower === 'html' || paramNameLower === 'message') {
        params[paramName] = `{{${currentVariable}}}`
        continue
      }

      // Google Sheets specific parameters - use consistent naming with DSLWrapper
      // DSLWrapper generates inputs like "google_sheets_id" based on data source name
      if (paramNameLower.includes('spreadsheet_id') || paramNameLower.includes('sheet_id')) {
        // Match the DSLWrapper naming convention: {source}_id
        // Use inputBaseName derived from plugin_key (e.g., "google_sheets")
        const idInputName = inputBaseName ? `${inputBaseName}_id` : 'spreadsheet_id'
        params[paramName] = `{{inputs.${idInputName}}}`
        continue
      }

      if (paramNameLower.includes('sheet_name') || paramNameLower === 'sheet') {
        // Match the DSLWrapper naming convention: {source}_sheet_name
        const sheetInputName = inputBaseName ? `${inputBaseName}_sheet_name` : 'sheet_name'
        params[paramName] = `{{inputs.${sheetInputName}}}`
        continue
      }

      if (paramNameLower === 'rows' || paramNameLower === 'data' || paramNameLower === 'values') {
        params[paramName] = `{{${currentVariable}}}`
        continue
      }

      if (paramNameLower === 'range') {
        params[paramName] = 'A:Z' // Default range
        continue
      }

      // Channel-based messaging parameters (works for any messaging plugin: Slack, Discord, Teams, etc.)
      if (paramNameLower.includes('channel')) {
        params[paramName] = deliveryRule.recipient || `{{inputs.channel}}`
        continue
      }

      // Message content parameters (generic for any messaging plugin)
      if (paramNameLower === 'text' || paramNameLower === 'message') {
        params[paramName] = `{{${currentVariable}}}`
        continue
      }

      // Use schema default if available
      if (paramSchema.default !== undefined) {
        params[paramName] = paramSchema.default
        continue
      }

      // Common optional parameters
      if (paramSchema.required !== true) {
        continue // Skip non-required params that we don't have values for
      }
    }

    return params
  }

  private generateStepId(prefix: string, ctx: CompilerContext): string {
    // Use simple step1, step2, etc. pattern instead of descriptive prefixes
    const id = `step${ctx.stepCounter}`
    ctx.stepCounter++
    return id
  }

  /**
   * Generate step metadata (id, name, step_id) for execution layer
   * Execution layer requires all three fields
   */
  /**
   * Compile summary statistics calculations
   *
   * Generates transform steps to calculate summary statistics like:
   * - total_amount: Sum of all amounts
   * - count: Number of records
   * - average_amount: Average of amounts
   *
   * This is plugin-agnostic and works by inferring the aggregation type
   * from the statistic name (e.g., "total_X" → sum, "average_X" → avg, "count" → count)
   *
   * NOTE: Skips meta-statistics that describe the process (e.g., number_of_emails_scanned)
   * rather than aggregating actual data fields.
   */
  private compileSummaryStats(
    summaryStats: string[],
    dataVariable: string,
    ctx: CompilerContext
  ): any[] {
    const steps: any[] = []

    // Meta-stat patterns that should be skipped - these describe the process, not data fields
    const metaStatPatterns = [
      /^number_of_/i,           // number_of_emails_scanned, number_of_pdfs_processed, etc.
      /^num_/i,                 // num_emails, num_files, etc.
      /_scanned$/i,             // emails_scanned, files_scanned
      /_processed$/i,           // pdfs_processed, items_processed
      /_extracted$/i,           // rows_extracted, data_extracted
      /_with_/i,                // rows_with_need_review, items_with_errors
      /^total_records$/i,       // Use 'count' instead
      /^record_count$/i,        // Use 'count' instead
    ]

    for (const stat of summaryStats) {
      const statLower = stat.toLowerCase()

      // Skip meta-statistics that don't represent actual data fields
      const isMetaStat = metaStatPatterns.some(pattern => pattern.test(stat))
      if (isMetaStat) {
        this.log(ctx, `⚠ Skipping meta-statistic "${stat}" - not a data field aggregation`)
        continue
      }

      // Infer aggregation type and field from stat name
      let aggregationType: 'sum' | 'count' | 'average' | 'min' | 'max'
      let field: string | null = null

      if (statLower === 'count' || statLower === 'row_count' || statLower === 'total_count') {
        aggregationType = 'count'
        field = null // Count doesn't need a field
      } else if (statLower.startsWith('total_')) {
        aggregationType = 'sum'
        field = stat.substring(6) // Remove "total_" prefix
      } else if (statLower.startsWith('sum_')) {
        aggregationType = 'sum'
        field = stat.substring(4) // Remove "sum_" prefix
      } else if (statLower.startsWith('average_') || statLower.startsWith('avg_')) {
        aggregationType = 'average'
        field = statLower.startsWith('average_') ? stat.substring(8) : stat.substring(4)
      } else if (statLower.startsWith('min_')) {
        aggregationType = 'min'
        field = stat.substring(4)
      } else if (statLower.startsWith('max_')) {
        aggregationType = 'max'
        field = stat.substring(4)
      } else {
        // Default: assume it's a sum of the field
        aggregationType = 'sum'
        field = stat
      }

      const statMetadata = this.generateStepMetadata(
        `calc_${stat}`,
        `Calculate ${stat}`,
        ctx
      )

      // Build aggregation in the format expected by StepExecutor:
      // { aggregations: [{ field, operation, alias }] }
      const aggregation: any = {
        operation: aggregationType,
        alias: stat // Use the stat name as the result key
      }

      if (field) {
        aggregation.field = field
      }

      steps.push({
        ...statMetadata,
        type: 'transform',
        operation: 'aggregate',
        input: `{{${dataVariable}}}`,
        config: {
          aggregations: [aggregation]
        }
      })

      this.log(ctx, `✓ Generated ${aggregationType} aggregation for ${stat}${field ? ` on field "${field}"` : ''}`)
    }

    return steps
  }

  /**
   * Dynamically extract file type filter from AI operation context/instruction
   * Works for ANY file type mentioned (pdf, docx, xlsx, png, txt, etc.)
   * Returns filter config or null if no file types detected
   */
  private extractFileTypeFilter(aiOp: any): { extensions?: string[] } | null {
    const context = this.getContextString(aiOp.context).toLowerCase()
    const instruction = (aiOp.instruction || '').toLowerCase()
    const combinedText = `${context} ${instruction}`

    // Regex to match common file extensions
    // Matches patterns like: "PDF", "pdf", ".pdf", "PDFs", "Word doc", "Excel file", etc.
    const fileExtensionPatterns = [
      /\b(pdf)s?\b/gi,
      /\b(docx?)s?\b/gi,
      /\b(xlsx?)s?\b/gi,
      /\b(pptx?)s?\b/gi,
      /\b(txt)s?\b/gi,
      /\b(csv)s?\b/gi,
      /\b(json)s?\b/gi,
      /\b(xml)s?\b/gi,
      /\b(png|jpe?g|gif|bmp|svg|webp)s?\b/gi,
      /\b(zip|rar|tar|gz)s?\b/gi,
      /\b(mp[34]|wav|flac|ogg)s?\b/gi,
      /\b(mp4|avi|mov|wmv|flv)s?\b/gi,
      /\.(pdf|docx?|xlsx?|pptx?|txt|csv|json|xml|png|jpe?g|gif|bmp|svg|webp|zip|rar|tar|gz|mp[34]|wav|flac|ogg|mp4|avi|mov|wmv|flv)\b/gi
    ]

    const detectedExtensions = new Set<string>()

    for (const pattern of fileExtensionPatterns) {
      const matches = combinedText.match(pattern)
      if (matches) {
        for (const match of matches) {
          // Normalize extension (remove dots, lowercase, remove trailing 's')
          let ext = match.toLowerCase().replace(/^\./, '').replace(/s$/, '')

          // Handle special cases (e.g., "jpeg" -> "jpg")
          if (ext === 'jpeg') ext = 'jpg'
          if (ext === 'doc') ext = 'docx' // Normalize to modern format
          if (ext === 'xls') ext = 'xlsx'
          if (ext === 'ppt') ext = 'pptx'

          detectedExtensions.add(ext)
        }
      }
    }

    if (detectedExtensions.size === 0) {
      return null
    }

    return {
      extensions: Array.from(detectedExtensions)
    }
  }

  private generateStepMetadata(prefix: string, description: string, ctx: CompilerContext) {
    const stepId = this.generateStepId(prefix, ctx)
    return {
      id: stepId,
      name: description,
      step_id: stepId
    }
  }

  /**
   * Normalize step inputs to prevent runtime type mismatches
   *
   * Common issue: Transform steps receive {{step2}} (whole StepOutput object)
   * instead of {{step2.emails}} (the actual array data)
   *
   * This method fixes those references at generation time by looking up
   * the plugin's output_schema to find the correct array field name.
   *
   * Different plugins return arrays under different field names:
   * - Gmail: { emails: [...] }
   * - Google Sheets: { values: [...] }
   * - Generic API: { data: [...] }
   */
  private normalizeStepInputs(steps: WorkflowStep[], ctx: CompilerContext): WorkflowStep[] {
    const stepMap = new Map<string, WorkflowStep>()
    // Build map using both id and step_id to handle different step formats
    steps.forEach(step => {
      if (step.id) stepMap.set(step.id, step)
      if ((step as any).step_id) stepMap.set((step as any).step_id, step)
    })

    let fixCount = 0

    const normalized = steps.map((step) => {
      // Only normalize transform steps (they need specific data types)
      if (step.type !== 'transform') {
        return step
      }

      const transformStep = step as any
      const input = transformStep.input

      // Check if input is a step reference
      if (typeof input === 'string') {
        // Pattern 1: Bare reference like {{step2}}
        const bareRefMatch = input.match(/^\{\{(step\d+)\}\}$/)
        // Pattern 2: Generic .data reference like {{step2.data}}
        const dataRefMatch = input.match(/^\{\{(step\d+)\.data\}\}$/)

        const stepRefMatch = bareRefMatch || dataRefMatch
        const needsFieldFix = !!stepRefMatch

        if (needsFieldFix && stepRefMatch) {
          const referencedStepId = stepRefMatch[1]
          const referencedStep = stepMap.get(referencedStepId)

          // If referenced step is an action, look up the output array field
          if (referencedStep && referencedStep.type === 'action') {
            const actionStep = referencedStep as any
            const plugin = actionStep.plugin
            const operation = actionStep.action || actionStep.operation

            // Get the array field name from plugin output_schema
            const arrayFieldName = this.getOutputArrayFieldName(plugin, operation)

            // Only fix if the field name is different from "data"
            if (arrayFieldName !== 'data' || bareRefMatch) {
              const oldInput = transformStep.input
              transformStep.input = `{{${referencedStepId}.${arrayFieldName}}}`

              this.log(ctx, `✓ Fixed input reference: ${oldInput} → ${transformStep.input} (plugin: ${plugin}, field: ${arrayFieldName})`)
              fixCount++
            }
          }
          // If referenced step is scatter-gather, append .results
          else if (referencedStep && (referencedStep as any).scatter) {
            const oldInput = transformStep.input
            transformStep.input = `{{${referencedStepId}.results}}`

            this.log(ctx, `✓ Fixed input reference: ${oldInput} → ${transformStep.input} (step: ${step.id})`)
            fixCount++
          }
        }
      }

      return step
    })

    if (fixCount > 0) {
      this.log(ctx, `✓ Normalized ${fixCount} step input(s) to prevent type errors`)
    } else {
      this.log(ctx, '✓ No input normalization needed - all references are correct')
    }

    return normalized
  }

  /**
   * Get the output array field name from a plugin's output_schema
   *
   * Uses SchemaAwareDataExtractor for consistent array field detection
   * across the entire pipeline.
   *
   * IMPORTANT: Some plugins return "single result objects" with nested arrays
   * (like research plugins returning {summary, key_points[], sources[]}).
   * These are NOT arrays of items to iterate - the whole object is the result.
   * Only plugins that return "arrays of items" (like email list) should drill into arrays.
   *
   * Falls back to "data" if no array field is found
   */
  private getOutputArrayFieldName(pluginName: string, operationName: string): string {
    const resolution = this.pluginResolver.resolveDataSource(pluginName, operationName as any)

    // Check if this plugin returns a "single result object" rather than "array of items"
    // These plugins have nested arrays for internal structure, NOT for iteration
    if (this.pluginReturnsSingleResultObject(pluginName)) {
      console.log(`[DeclarativeCompiler] Plugin ${pluginName} returns single result object, using 'data' as array field (no iteration into nested arrays)`)
      return 'data'
    }

    if (resolution.output_schema) {
      const analysis = analyzeOutputSchema(resolution.output_schema)

      if (analysis.primaryArrayField) {
        return analysis.primaryArrayField
      }

      // If the schema itself is an array type, return "data" as default
      if (resolution.output_schema.type === 'array') {
        return 'data'
      }
    }

    return 'data'
  }

  /**
   * Check if a plugin returns a "single result object" (not an array of items)
   *
   * Single result plugins (like research, AI) return ONE structured object.
   * Their nested arrays (key_points[], sources[]) are internal structure,
   * NOT items to iterate over in a scatter_gather loop.
   *
   * Array-of-items plugins (like email, spreadsheet) return a LIST of items
   * that ARE meant to be iterated in scatter_gather loops.
   */
  private pluginReturnsSingleResultObject(pluginName: string): boolean {
    const pluginDef = this.pluginResolver.getPluginDefinition(pluginName)

    if (!pluginDef) {
      return false
    }

    // Check plugin category - AI/research plugins return single result objects
    const category = pluginDef.plugin?.category?.toLowerCase() || ''
    const singleResultCategories = ['ai_research', 'ai', 'research', 'analysis']

    if (singleResultCategories.some(cat => category.includes(cat))) {
      return true
    }

    // Check plugin name patterns for research/AI plugins
    const pluginNameLower = pluginName.toLowerCase()
    const singleResultPatterns = ['research', 'chatgpt', 'openai', 'ai-', 'analysis']

    if (singleResultPatterns.some(pattern => pluginNameLower.includes(pattern))) {
      return true
    }

    return false
  }

  /**
   * Check if any AI transform operation explicitly produces HTML output
   *
   * This is used to skip redundant render_table steps when the AI
   * instruction already asks for HTML formatting (e.g., "format as HTML table")
   *
   * Detection covers:
   * 1. Explicit HTML mentions: "HTML table", "HTML format", "generate HTML"
   * 2. Format specifications: "format as HTML", "output as HTML", "render HTML"
   * 3. Structure requests: "structured HTML", "HTML with columns"
   * 4. HTML tags in instruction: "<table>", "<html>", "<body>", "<div>"
   * 5. Web formatting: "web-ready format", "email-ready HTML"
   */
  private didAIProduceHtmlOutput(ir: DeclarativeLogicalIR): boolean {
    if (!ir.ai_operations || ir.ai_operations.length === 0) {
      return false
    }

    // Check transform/generate operations for HTML output indicators
    const htmlProducingOps = ir.ai_operations.filter(op => {
      // Only consider transform and generate operations
      if (op.type !== 'transform' && op.type !== 'generate') {
        return false
      }

      const instruction = (op.instruction || '').toLowerCase()

      // Pattern 1: Explicit HTML mentions (word boundaries)
      const htmlMentions = [
        'html table',
        'html format',
        'html output',
        'html content',
        'html document',
        'html email',
        'html body',
        'html structure',
        'html report',
        'html summary',
        'html list',
        'html layout',
        'structured html',
        'formatted html',
        'styled html',
        'rich html'
      ]

      // Pattern 2: Action + HTML combinations
      const actionHtmlPatterns = [
        'generate html',
        'create html',
        'produce html',
        'build html',
        'construct html',
        'render html',
        'output html',
        'return html',
        'format as html',
        'format to html',
        'format into html',
        'convert to html',
        'transform to html',
        'render as html',
        'output as html',
        'present as html',
        'display as html'
      ]

      // Pattern 3: HTML tags in instruction (user explicitly wants HTML)
      const htmlTags = [
        '<table>',
        '<table ',
        '<html>',
        '<html ',
        '<body>',
        '<body ',
        '<div>',
        '<div ',
        '<tr>',
        '<td>',
        '<th>',
        '<ul>',
        '<ol>',
        '<li>'
      ]

      // Pattern 4: Web/email ready formatting (implies HTML)
      const webPatterns = [
        'email-ready',
        'web-ready',
        'email ready',
        'web ready',
        'email friendly',
        'web friendly'
      ]

      // Check all patterns
      const allPatterns = [...htmlMentions, ...actionHtmlPatterns, ...htmlTags, ...webPatterns]
      return allPatterns.some(pattern => instruction.includes(pattern))
    })

    return htmlProducingOps.length > 0
  }

  /**
   * Dynamically detect if plugin operation returns a 2D array (array of arrays)
   *
   * 2D arrays (like Google Sheets values) need to be converted to objects
   * using the first row as headers before field-based access works.
   *
   * Uses SchemaAwareDataExtractor for consistent detection across the pipeline.
   *
   * @returns { is2DArray: boolean, arrayFieldName: string } - whether it's a 2D array and which field
   */
  private detectOutputIs2DArray(pluginName: string, operationName: string): { is2DArray: boolean, arrayFieldName: string } {
    const resolution = this.pluginResolver.resolveDataSource(pluginName, operationName as any)

    if (resolution.output_schema) {
      const analysis = analyzeOutputSchema(resolution.output_schema)

      if (analysis.is2DArray && analysis.primaryArrayField) {
        return { is2DArray: true, arrayFieldName: analysis.primaryArrayField }
      }
    }

    return { is2DArray: false, arrayFieldName: 'data' }
  }

  /**
   * Map IR column names to actual plugin output field names
   *
   * The IR may use user-friendly column names like "sender_email" but
   * the plugin output_schema uses actual field names like "from".
   *
   * This method performs fuzzy matching to resolve the mapping.
   */
  private mapColumnsToPluginFields(
    columns: string[],
    pluginName: string,
    operationName: string,
    ctx: CompilerContext
  ): string[] {
    // Get the plugin's output schema
    const resolution = this.pluginResolver.resolveDataSource(pluginName, operationName as any)

    if (!resolution.output_schema) {
      this.log(ctx, `No output_schema found for ${pluginName}.${operationName}, using original columns`)
      return columns
    }

    // Find the array field that contains the actual item schema
    let itemSchema: Record<string, any> | null = null

    if (resolution.output_schema.type === 'object' && resolution.output_schema.properties) {
      for (const [fieldName, fieldDef] of Object.entries(resolution.output_schema.properties as Record<string, any>)) {
        if (fieldDef.type === 'array' && fieldDef.items?.properties) {
          itemSchema = fieldDef.items.properties
          break
        }
      }
    }

    if (!itemSchema) {
      this.log(ctx, `No item schema found for ${pluginName}.${operationName}, using original columns`)
      return columns
    }

    // Get the actual field names from the schema
    const actualFields = Object.keys(itemSchema)

    // Build semantic mappings dynamically from schema field descriptions
    // This is plugin-agnostic - no hardcoded plugin names
    const columnMappings: Record<string, string[]> = this.buildSemanticMappingsFromSchema(itemSchema)

    // ✅ Build field redirects from schema descriptions
    // Parses hints like "Usually empty - use 'snippet' field instead"
    const fieldRedirects = this.extractFieldRedirectsFromSchema(itemSchema)

    // Map each column
    const mappedColumns: string[] = []

    for (const column of columns) {
      const lowerColumn = column.toLowerCase().replace(/[_\s-]/g, '')

      // 0. ✅ FIRST: Check if this is an AI output field - keep as-is, don't map to plugin fields
      // AI operations generate fields with specific names (e.g., "Sender", "Priority")
      // These should NOT be mapped to plugin fields (e.g., "from") as the AI outputs them directly
      if (ctx.aiOutputFields && ctx.aiOutputFields.some(f => f.toLowerCase() === column.toLowerCase())) {
        this.log(ctx, `✓ Keeping AI output column "${column}" as-is`)
        mappedColumns.push(column)
        continue
      }

      // 1. Check for schema-based redirects (e.g., "body" → "snippet" based on description hints)
      const redirect = fieldRedirects[column.toLowerCase()]
      if (redirect && actualFields.includes(redirect)) {
        this.log(ctx, `✓ Schema redirect: "${column}" → "${redirect}" (from field description hint)`)
        mappedColumns.push(redirect)
        continue
      }

      // 2. Direct match (case-insensitive)
      const directMatch = actualFields.find(
        f => f.toLowerCase() === column.toLowerCase() || f.toLowerCase().replace(/[_\s-]/g, '') === lowerColumn
      )
      if (directMatch) {
        mappedColumns.push(directMatch)
        continue
      }

      // 3. Check common mappings
      const mappingCandidates = columnMappings[column.toLowerCase()] || columnMappings[column]
      if (mappingCandidates) {
        const mapped = mappingCandidates.find(candidate =>
          actualFields.some(f => f.toLowerCase() === candidate.toLowerCase())
        )
        if (mapped) {
          const actualField = actualFields.find(f => f.toLowerCase() === mapped.toLowerCase())!
          this.log(ctx, `✓ Mapped column "${column}" → "${actualField}" (plugin: ${pluginName})`)
          mappedColumns.push(actualField)
          continue
        }
      }

      // 4. Fuzzy match - look for partial matches
      const fuzzyMatch = actualFields.find(f => {
        const lowerField = f.toLowerCase()
        return lowerColumn.includes(lowerField) || lowerField.includes(lowerColumn)
      })
      if (fuzzyMatch) {
        this.log(ctx, `✓ Fuzzy mapped column "${column}" → "${fuzzyMatch}" (plugin: ${pluginName})`)
        mappedColumns.push(fuzzyMatch)
        continue
      }

      // 5. No match found in plugin schema
      // IMPORTANT: Don't skip! Keep the original column name - it may be:
      // - An AI output field with a semantic display name (e.g., "CTA (what to do)" → "cta")
      // - A field that will be added by AI processing
      // The render/map step will try to extract data using this name, and fallback logic will handle it
      this.log(ctx, `⚠ No plugin match for "${column}" - keeping as-is (may be AI field or transformed data)`)
      mappedColumns.push(column)
    }

    return mappedColumns
  }

  /**
   * Build semantic column mappings dynamically from schema field descriptions
   * This is completely plugin-agnostic - extracts semantic hints from field metadata
   */
  private buildSemanticMappingsFromSchema(itemSchema: Record<string, any>): Record<string, string[]> {
    const mappings: Record<string, string[]> = {}

    // Semantic keyword categories that map user-friendly names to schema fields
    const semanticCategories: Record<string, string[]> = {
      // Sender/From patterns
      'sender': ['from', 'sender', 'author', 'creator', 'owner', 'submitted_by'],
      'sender_email': ['from', 'sender_email', 'author_email', 'email'],
      // Content patterns
      'content': ['body', 'content', 'text', 'message', 'description', 'snippet'],
      'full_text': ['body', 'content', 'text', 'full_text', 'raw_text'],
      'preview': ['snippet', 'preview', 'summary', 'excerpt'],
      // ID patterns
      'id': ['id', 'message_id', 'record_id', 'item_id', 'thread_id', 'uid'],
      'link': ['id', 'url', 'link', 'permalink', 'href'],
      // Date patterns
      'date': ['date', 'created_at', 'timestamp', 'received_at', 'sent_at', 'modified_at'],
      'received_date': ['date', 'received_at', 'received_date', 'timestamp'],
      // Subject/Title patterns
      'subject': ['subject', 'title', 'name', 'heading', 'header'],
      'title': ['title', 'name', 'subject', 'label'],
      // Status patterns
      'status': ['status', 'state', 'stage', 'phase'],
      // Index patterns
      'row': ['row', 'index', 'row_index', 'row_number', 'line'],
      // Generic patterns
      'name': ['name', 'full_name', 'display_name', 'title', 'label'],
      'email': ['email', 'email_address', 'mail', 'e_mail'],
    }

    // For each field in schema, find semantic matches
    for (const [fieldName, fieldDef] of Object.entries(itemSchema)) {
      const lowerField = fieldName.toLowerCase()
      const description = (fieldDef as any).description?.toLowerCase() || ''

      // Add the field itself as a mapping target
      for (const [semanticName, candidates] of Object.entries(semanticCategories)) {
        // Check if field name matches any candidate - use unshift() for priority
        // Field name matches (e.g., 'from' matching 'from') are more reliable
        if (candidates.some(c => lowerField === c || lowerField.includes(c))) {
          if (!mappings[semanticName]) mappings[semanticName] = []
          if (!mappings[semanticName].includes(fieldName)) {
            mappings[semanticName].unshift(fieldName)
          }
        }

        // Check if description contains semantic hints - use push() for lower priority
        // Description matches can be unreliable (e.g., "Email subject" matching 'email')
        if (description && candidates.some(c => description.includes(c))) {
          if (!mappings[semanticName]) mappings[semanticName] = []
          if (!mappings[semanticName].includes(fieldName)) {
            mappings[semanticName].push(fieldName)
          }
        }
      }

      // Also add direct field name as mapping key
      if (!mappings[lowerField]) mappings[lowerField] = []
      if (!mappings[lowerField].includes(fieldName)) {
        mappings[lowerField].push(fieldName)
      }
    }

    return mappings
  }

  /**
   * Extract field redirects from schema descriptions
   * Parses patterns like:
   *   - "Usually empty - use 'snippet' field instead"
   *   - "Empty in search results, use 'preview' instead"
   *   - "Prefer 'content' field for full text"
   *
   * Returns a map of field names to their preferred alternatives
   */
  private extractFieldRedirectsFromSchema(itemSchema: Record<string, any>): Record<string, string> {
    const redirects: Record<string, string> = {}

    // Patterns to detect redirect hints in descriptions
    // Format: field → suggested_alternative
    const redirectPatterns = [
      // "use 'fieldname' field instead" or "use 'fieldname' instead"
      /use\s+['"]?(\w+)['"]?\s+(?:field\s+)?instead/i,
      // "prefer 'fieldname' field" or "prefer 'fieldname'"
      /prefer\s+['"]?(\w+)['"]?\s*(?:field)?/i,
      // "see 'fieldname' for" or "see 'fieldname'"
      /see\s+['"]?(\w+)['"]?\s+for/i,
      // "Usually empty... use 'fieldname'"
      /(?:usually|often|typically)\s+empty.*?['"](\w+)['"]/i,
    ]

    for (const [fieldName, fieldDef] of Object.entries(itemSchema)) {
      const description = (fieldDef as any).description || ''

      if (!description) continue

      // Check if description suggests this field should redirect to another
      for (const pattern of redirectPatterns) {
        const match = description.match(pattern)
        if (match && match[1]) {
          const suggestedField = match[1].toLowerCase()
          // Only add redirect if the suggested field exists in schema
          if (itemSchema[suggestedField]) {
            redirects[fieldName.toLowerCase()] = suggestedField
            break
          }
        }
      }
    }

    return redirects
  }

  private log(ctx: CompilerContext, message: string): void {
    ctx.logs.push(message)
    console.log(`[DeclarativeCompiler] ${message}`)
  }

  /**
   * Safely extract a string from context field which may be string, array, or other types
   * Handles cases where LLM generates non-string context values
   */
  private getContextString(context: unknown): string {
    if (typeof context === 'string') {
      return context
    }
    if (Array.isArray(context)) {
      return context.join(' ')
    }
    if (context && typeof context === 'object') {
      return JSON.stringify(context)
    }
    return ''
  }

  /**
   * Extract list of plugins used in the compiled workflow
   */
  private extractPluginsUsed(steps: WorkflowStep[]): string[] {
    const plugins = new Set<string>()

    const extractFromStep = (step: WorkflowStep) => {
      // Check action steps for plugin field
      if (step.type === 'action' && (step as any).plugin) {
        plugins.add((step as any).plugin)
      }

      // Check nested steps (loops, scatter, conditionals)
      if ((step as any).loopSteps) {
        (step as any).loopSteps.forEach(extractFromStep)
      }
      if ((step as any).steps) {
        (step as any).steps.forEach(extractFromStep)
      }
      if ((step as any).scatter?.steps) {
        (step as any).scatter.steps.forEach(extractFromStep)
      }
      if ((step as any).workflowSteps) {
        (step as any).workflowSteps.forEach(extractFromStep)
      }
    }

    steps.forEach(extractFromStep)
    return Array.from(plugins)
  }

  /**
   * Check if a data source is a reference data source for deduplication
   * Supports multiple role aliases that LLMs might use:
   * - "reference" (original)
   * - "lookup" (common semantic term)
   * - "existing_records" (descriptive)
   * - "deduplicate" (explicit intent)
   * - "reference_store" (storage intent)
   */
  private isReferenceDataSource(ds: DataSource): boolean {
    if (!ds.role) return false

    const roleLower = ds.role.toLowerCase()
    const referenceKeywords = [
      'reference',
      'lookup',
      'existing_records',
      'deduplicate',
      'reference_store',
      'dedup',
      'existing',
      'check_against',
      'dedupe'  // Added: common shorthand
    ]

    // Check if any reference keyword is contained in the role string
    // (not the other way around - role can be a sentence like "Read existing rows to dedupe...")
    return referenceKeywords.some(keyword => roleLower.includes(keyword))
  }

  /**
   * Wave 9: Check if IR has duplicate_records edge case
   * This indicates the workflow needs deduplication logic
   */
  private hasDuplicateRecordsEdgeCase(ir: DeclarativeLogicalIR): boolean {
    if (!ir.edge_cases || ir.edge_cases.length === 0) return false
    return ir.edge_cases.some(ec => ec.condition === 'duplicate_records')
  }

  /**
   * Detect if AI operation needs file content to be fetched before processing.
   *
   * This is used when working with file attachments (PDFs, images, documents, etc.)
   * from email, JIRA, or other sources where API returns only metadata initially.
   *
   * Detection uses the `input_description` field from the semantic plan,
   * which describes what data the AI will process (e.g., "PDF receipt attachments").
   *
   * Examples where this is true:
   * - PDF invoice extraction (AI needs PDF content)
   * - Image OCR/analysis (AI needs image data)
   * - Document parsing (AI needs document content)
   *
   * Examples where this is false:
   * - Working with text fields already in API response
   * - Processing structured data from API
   */
  private detectNeedsContentFetch(_context: string, aiOp: AIOperation): boolean {
    // Detect from input_description - this field describes what data the AI processes
    // Examples: "PDF receipt attachments", "invoice images", "document files"
    const inputDesc = (aiOp.input_description || '').toLowerCase()
    const context = this.getContextString(aiOp.context).toLowerCase()

    // File type indicators that require content fetching
    const fileTypeIndicators = [
      'pdf', 'image', 'photo', 'picture', 'screenshot',
      'document', 'docx', 'xlsx', 'receipt', 'invoice', 'scan'
    ]

    // Check both input_description and context for file type indicators
    const textToCheck = `${inputDesc} ${context}`
    return fileTypeIndicators.some(indicator => textToCheck.includes(indicator))
  }

  /**
   * Detect workflow pattern type for metrics
   */
  private detectPatternType(ir: DeclarativeLogicalIR, features: any): string {
    if (features.multiDestination) return 'multi-destination'
    if (features.hasGrouping) return 'grouped'
    if (features.hasPartitions) return 'partitioned'
    if (features.hasAI) return 'ai-enhanced'
    if (features.hasDeduplication) return 'deduplicated'
    if (features.hasFilters) return 'filtered'
    return 'linear'
  }

  // ==========================================================================
  // Execution Constraints & Edge Cases
  // ==========================================================================

  /**
   * Apply execution constraints to workflow steps
   * Maps IR execution_constraints to PILOT step properties (retryPolicy, timeout)
   */
  private applyExecutionConstraints(
    steps: WorkflowStep[],
    constraints: import('../logical-ir/schemas/declarative-ir-types').ExecutionConstraints,
    ctx: CompilerContext
  ): void {
    const { retry, timeout, concurrency } = constraints

    // Build PILOT retryPolicy from IR retry config
    let retryPolicy: any = undefined
    if (retry) {
      retryPolicy = {
        maxRetries: retry.max_attempts - 1, // PILOT uses maxRetries, IR uses max_attempts
        backoffMs: retry.initial_delay_ms || 1000,
        backoffMultiplier: retry.backoff_strategy === 'exponential' ? 2 : 1,
        retryableErrors: retry.retry_on_errors || ['rate_limit', 'timeout', 'server_error']
      }
      this.log(ctx, `✓ Applied retry policy: ${retry.max_attempts} attempts, ${retry.backoff_strategy} backoff`)
    }

    // Apply constraints to all action steps
    for (const step of steps) {
      if (step.type === 'action') {
        // Apply retry policy
        if (retryPolicy) {
          (step as any).retryPolicy = retryPolicy
        }

        // Apply step timeout
        if (timeout?.step_timeout_ms) {
          (step as any).timeout = timeout.step_timeout_ms
        }
      }

      // Apply concurrency to scatter-gather steps
      if (step.type === 'scatter_gather' && concurrency?.max_concurrent_operations) {
        if ((step as any).scatter) {
          (step as any).scatter.maxConcurrency = concurrency.max_concurrent_operations
        }
      }

      // Recursively apply to nested steps
      if ((step as any).scatter?.steps) {
        this.applyExecutionConstraints((step as any).scatter.steps, constraints, ctx)
      }
    }
  }

  /**
   * Apply edge case handlers to workflow steps
   * Maps IR edge_cases to PILOT on_error handlers
   */
  private applyEdgeCaseHandlers(
    steps: WorkflowStep[],
    edgeCases: import('../logical-ir/schemas/declarative-ir-types').EdgeCase[],
    ctx: CompilerContext
  ): void {
    // Map edge case conditions to step types
    for (const edgeCase of edgeCases) {
      let onErrorHandler: any = undefined

      // Map edge case action to PILOT on_error action
      switch (edgeCase.action) {
        case 'skip_execution':
          onErrorHandler = { action: 'continue', log_error: true }
          break
        case 'retry':
          onErrorHandler = { action: 'retry', log_error: true }
          break
        case 'send_empty_result_message':
          onErrorHandler = { action: 'continue', log_error: true }
          // Note: The actual empty message sending is handled by delivery pattern
          break
        case 'alert_admin':
          onErrorHandler = { action: 'stop', log_error: true }
          break
        case 'use_default_value':
          onErrorHandler = { action: 'continue', log_error: true }
          break
        default:
          onErrorHandler = { action: 'continue', log_error: true }
      }

      // Apply to relevant steps based on condition
      switch (edgeCase.condition) {
        case 'api_error':
        case 'rate_limit_exceeded':
          // Apply to all action steps
          for (const step of steps) {
            if (step.type === 'action') {
              (step as any).on_error = onErrorHandler
            }
          }
          break

        case 'empty_data_source':
        case 'no_rows_after_filter':
          // Apply to data source and filter steps
          for (const step of steps) {
            if (step.type === 'action' || step.type === 'transform') {
              (step as any).on_error = onErrorHandler
            }
          }
          break

        case 'missing_required_field':
          // Apply to transform steps
          for (const step of steps) {
            if (step.type === 'transform') {
              (step as any).on_error = onErrorHandler
            }
          }
          break
      }

      this.log(ctx, `✓ Applied edge case handler: ${edgeCase.condition} → ${edgeCase.action}`)
    }
  }

  /**
   * Log warnings for IR fields that are present but not compiled
   * Helps users understand what's being ignored
   */
  private logIgnoredIRFields(ir: DeclarativeLogicalIR, ctx: CompilerContext): void {
    const warnings: string[] = []

    // Check for conditionals (not yet fully implemented)
    if (ir.conditionals && ir.conditionals.length > 0) {
      warnings.push(`IR has ${ir.conditionals.length} conditional(s) - conditional branching compilation is limited`)
    }

    // Check for file_operations (not yet implemented)
    if ((ir as any).file_operations && (ir as any).file_operations.length > 0) {
      warnings.push(`IR has ${(ir as any).file_operations.length} file_operation(s) - file generation not yet implemented`)
    }

    // Check for unused rendering fields
    if (ir.rendering) {
      if (ir.rendering.template) {
        warnings.push('IR has rendering.template - custom templates not yet implemented')
      }
      if (ir.rendering.engine && ir.rendering.engine !== 'handlebars') {
        warnings.push(`IR has rendering.engine="${ir.rendering.engine}" - only handlebars is supported`)
      }
    }

    // Log warnings
    for (const warning of warnings) {
      ctx.warnings.push(warning)
      console.warn(`[DeclarativeCompiler] ⚠️ ${warning}`)
    }
  }
}

// ============================================================================
// Export Helper Function
// ============================================================================

export async function compileDeclarativeIR(
  ir: DeclarativeLogicalIR,
  pluginManager?: PluginManagerV2
): Promise<CompilationResult> {
  const compiler = new DeclarativeCompiler(pluginManager)
  return compiler.compile(ir)
}
